---
title: "Machine learning Data Competition 2020"
subtitle: 'Report I.'
author: Shreyasvi Natraj
output:
  pdf_document:
        number_sections: true
  html_document:
    df_print: paged
urlcolor: blue
---


```{r setup, include=FALSE}
# import knitr (super important!!!)
library(knitr)

# import kableExtra (to print nice tables)
library(kableExtra)

# set global options (can be modified locally)
knitr::opts_chunk$set(warning=FALSE, message=FALSE, cache=TRUE, fig.show = 'hold', 
                      fig.align = 'center', out.width = "75%")

```

\section{Introduction}
For the given data competition, we were provided with data pertaining to previous adverstisement campaigns as well as demographics of users who have been a part of the survey conducted. 

For the objective of the given task, we are required to train a model that can be used in order to predict whether if a user is likely to have a "conversion" where a "conversion" refers to the user clicking on the advertisement and subscribing to the service.

Since the data provided is used in order to predict a categorical variable i.e. "conversion/y", we planned to use do a quick an dirty implementation of the following models to check for their accuracy:

*K-Nearest Neighbours
*Random Forest
*LDA, QDA & C5.0
*Supported Vector Machines
*Logistic Regression

We initially carried out with exploratory data analysis for the data provided to us by considering na values as NaN values.

\section{Exploratory data analysis}
We observed from this that it would not be a good idea to not consider the na values as NaN but as a separate level. However, based on similarity in between the classes, we can merge different levels of a factor into lesser number of levels so they are easier for our model to interpret.
\subsection{Interpretation}
We also carried out the same process of data analysis after converting categorical variables into inteager format which tend to show a similar fashion to the current analysis being carried out. However, we replaced "na" values with 0 which made the data much more consistent.
We also observed that `time_spent`,`outcome_old` and `X3` tends to hold a very high significance when predicting conversion `y`.
\subsection{Plots}
```{r Data Distribution, fig.cap="Data Distribution"}
#install.packages("DataExplorer")
library(tidyverse)
library(DataExplorer)
dataset = read.csv('train.csv')
dataset[ dataset == "na" ] <- NA

web<- dataset
glimpse(web)
#To go with glimpse(), DataExplorer itself has got a function called introduce()
introduce(web)

#The same introduce() could also be plotted in a pretty graph.
plot_intro(web,  ggtheme = theme_dark(),
             title = "EDA with Data Explorer",
             )
```

```{r Missing variables, fig.cap="Missing Columns"}
plot_missing(web,  
             ggtheme = theme_dark(),
             title = " Features missing from the whole observations",
             )
```

###EDA for Continuous variables
```{r Continous variables, fig.cap="Continous Variables"}
##for univariate

DataExplorer::plot_histogram(web,  
             ggtheme = theme_dark(),
             title = " Histogram of continuous features",
             )

plot_density(web,  
             ggtheme = theme_dark(),
             title = " Density of continuous features",
             )  # age, time_spent, X4 are right skewed
#outcome old hase a mode which is na-> remove this category? or remove this variable??

plot_bar(web,  
             ggtheme = theme_dark(),
             title = " Density of continuous features",
             )  ##VISUALIZE DATA WHEN X2=0 AND =1 (subsetting)

                    
                    a<- filter(web, web$X1==0)
                    b<- filter(web, web$X1==1)
                    plot_bar(a)
                    plot_bar(b)
                    plot_density(a,
                                 title = " a")
                    plot_density(b,
                                 title = " b") 

##for bivariate
  plot_boxplot(web, by= 'day' , ncol = 1,   
             ggtheme = theme_dark(),
             title = " Boxplot of continuous features by day",
             )
```
###Correlation Plot
```{r Correlation Plot, fig.cap="Correlation Plot"}
##autocorr plot
plot_correlation(web, cor_args = list( 'use' = 'complete.obs'),  
             ggtheme = theme_dark(),
             title = " Autocorr Plot",
             )

##continurous correlation plot
plot_correlation(web, type = 'c',cor_args = list( 'use' = 'complete.obs'),  
             ggtheme = theme_dark(),
             title = " Continuous corr Plot",
             )  

```

###EDA for Categorical`
```{r Categorical Features Plot, fig.cap="Categorical Features Plot"}
plot_bar(a, maxcat = 390, parallel = FALSE,  
             ggtheme = theme_dark(),
             title = " Categorical Features Plot",
             )
```

For more sophisticated graphs, that span over multiple pages, see function
`ggarrange()` from `ggpubr` package (see [link](http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/81-ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page/)).

For good-looking colors, have a look at the Paul Tol's palette <https://personal.sron.nl/~pault/>.



\subsection{Tables}
To display a table, look at the `kable()` function from `knitr` package. Also,
consider the `kableExtra` package for more sophisticated options (see [link](https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf)).
In Table \ref{tab:tblname},
we show an example that uses both `kable` and `kableExtra`.

You can reference a table by putting the code `\\label{tab:tblname}` inside the
caption. See code below. Then, you see that the reference works (see Table \ref{tab:tblname}).
```{r table1, fig.cap="Simple caption."}
# Prepare data to put in the table
dat2 <- mtcars %>% 
  group_by(cyl) %>% 
  summarise(Average = mean(mpg), Max = max(mpg), Sqrt = sum(sqrt(mpg)))

# Print table
capt <- paste("\\label{tab:tblname}Average and ",
              "maximum miles per gallon for each number of cylindyers class.")
kable(dat2,
      format = "latex",
      longtable = F,
      booktabs = T,
      digits = 2,
      caption = capt) %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

If you want to manually insert the values in the table, you can do it, too 
(see Table \ref{tab:tab2}).

\begin{table}[H]
\caption{\label{tab:tab2}Number of different levels and the number of predictors that have this amount of levels.}
\centering
\begin{tabular}{lcccc}
\toprule
 & Col 1 & Col 2 & Col 3 & Col 4\\
\midrule
\rowcolor{gray!6}
Number of different values & 2 & 4 & 12 & $> 300$\\
Number of predictors & ... & ... & ... & ...\\
\bottomrule
\end{tabular}
\end{table}



\section{Models}

\subsection{Linear model}

The first approach is to fit a linear model, that is, the regression function $f$ in \eqref{eq1} is assumed to be of the form
$$
  f(X) = \beta_0 + \sum_{j=1}^p \beta_j X_j,
$$
where $\beta_j\in \mathbb R$ is the coefficient of the $j$th predictor.

Fitting this model to the training data, we obtain a predictive model $\hat f_{LM}$. The training and cross-validation error of this model can be found in Table \ref{tab_res}....

There are ... statistically significant predictors... 

Possible meaning and interpretation of some predictors

\section{Validation}

We implement a cross-validation ....


\section{Results}

\subsection{Preliminary Implementation}
We started by dividing the set into 75-35 percent split and running them through different machine learning models in a crude manner to check out of the box which model tends to perform best on the given dataset.
```{r}
#Support Vector Machine
rm(list=ls())
# Importing the dataset
dataset = read.csv('train.csv')

dataset$days_elapsed_old[dataset$days_elapsed_old<1] <- 0
dataset[ dataset == "na" ] <- NA

#Factor like columns
dataset$job=as.integer(as.factor(dataset$job))
dataset$marital=as.integer(as.factor(dataset$marital))
dataset$education=as.integer(as.factor(dataset$education))
dataset$device=as.integer(as.factor(dataset$device))
dataset$outcome_old=as.integer(as.factor(dataset$outcome_old))
dataset[is.na(dataset)] <- 0

# Encoding the target feature as factor
dataset$y= factor(dataset$y, levels = c(0, 1))

# Splitting the dataset into the Training set and Test set
#install.packages('caTools')
library(caTools)
set.seed(123)

split = sample.split(dataset$y, SplitRatio = 0.75)

training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling
training_set[-17] = scale(training_set[-17])
test_set[-17] = scale(test_set[-17])

# Fitting SVM to the Training set
#install.packages('e1071')
library(e1071)
classifier = svm(formula = y ~ .,
                 data = training_set,
                 type = 'C-classification',
                 kernel = 'radial')

# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-17],drop=TRUE)
y_pred

# Making the Confusion Matrix
cm = table(test_set[, 17], y_pred)
cm
print("======================================SVM=====================================")
library(ggplot2)
library(lattice)
library(caret)
confusionMatrix(cm)

#================================================================================================================
rm(list=ls())
#Random Forest Classification

# Importing the dataset
dataset = read.csv('train.csv')

dataset$days_elapsed_old[dataset$days_elapsed_old<1] <- 0
dataset[ dataset == "na" ] <- NA

#Factor like columns
dataset$job=as.integer(as.factor(dataset$job))
dataset$marital=as.integer(as.factor(dataset$marital))
dataset$education=as.integer(as.factor(dataset$education))
dataset$device=as.integer(as.factor(dataset$device))
dataset$outcome_old=as.integer(as.factor(dataset$outcome_old))
dataset[is.na(dataset)] <- 0

# Encoding the target feature as factor
dataset$y = factor(dataset$y, levels = c(0, 1))

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$y, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling #for higher resolution visualisation only we are using feature scaling,RF doesnt need feature scaling
training_set[-17] = scale(training_set[-17])
test_set[-17] = scale(test_set[-17])

# Fitting Random Forest Classification to the Training set
#install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-17],
                          y = training_set$y)#,                           ntree = 700)                 

# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-17])
y_pred

# Making the Confusion Matrix
cm = table(test_set[, 17], y_pred)
cm
print("=====================================Random Forest=====================================")
library(ggplot2)
library(lattice)
library(caret)
confusionMatrix(cm)


#=================================================================================================================
# Logistic Regression
rm(list=ls())
# Importing the dataset
dataset = read.csv('train.csv')

dataset$days_elapsed_old[dataset$days_elapsed_old<1] <- 0
dataset[ dataset == "na" ] <- NA

#Factor like columns
dataset$job=as.integer(as.factor(dataset$job))
dataset$marital=as.integer(as.factor(dataset$marital))
dataset$education=as.integer(as.factor(dataset$education))
dataset$device=as.integer(as.factor(dataset$device))
dataset$outcome_old=as.integer(as.factor(dataset$outcome_old))
dataset[is.na(dataset)] <- 0

# Encoding the target feature as factor

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$y, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling
training_set[,1:16] = scale(training_set[,1:16])
test_set[-17] = scale(test_set[-17]) #removes third column alone

#fitting logistic regression to the training set
classifier = glm(formula = y ~ .,
                 family = binomial, #for logistic reg mention binomial
                 data = training_set)

#predicting the test set results
prob_pred = predict(classifier, type = 'response',newdata = test_set[-17])#use type = response for logistic reg
prob_pred                                                          #that will give the prob listed in the single vector
y_pred = ifelse(prob_pred > 0.5, 1, 0)
y_pred

#making the confusion matrix
cm = table(test_set[,17], y_pred)
cm

print("=====================================Logistic Regression=====================================")

library(ggplot2)
library(lattice)
library(caret)
confusionMatrix(cm)

#=================================================================================================================
#Naive Bayes
rm(list=ls())
# Importing the dataset
dataset = read.csv('train.csv')

dataset$days_elapsed_old[dataset$days_elapsed_old<1] <- 0
dataset[ dataset == "na" ] <- NA

#Factor like columns
dataset$job=as.integer(as.factor(dataset$job))
dataset$marital=as.integer(as.factor(dataset$marital))
dataset$education=as.integer(as.factor(dataset$education))
dataset$device=as.integer(as.factor(dataset$device))
dataset$outcome_old=as.integer(as.factor(dataset$outcome_old))
dataset[is.na(dataset)] <- 0

# Encoding the target feature as factor
dataset$y = factor(dataset$y, levels = c(0, 1)) #labels /levels -both are same

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$y, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling
training_set[-17] = scale(training_set[-17])
test_set[-17] = scale(test_set[-17])

# Fitting Naive Bayes to the Training set
library(e1071)
classifier = naiveBayes(x = training_set[-17],
                        y = training_set$y) 

# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-17])

# Making the Confusion Matrix
cm = table(test_set[, 17], y_pred)

print("=====================================Naive Bayes=====================================")

library(ggplot2)
library(lattice)
library(caret)
confusionMatrix(cm)


#=================================================================================================================
#Decision Tree Classification

rm(list=ls())

# Importing the dataset
dataset = read.csv('train.csv')

dataset$days_elapsed_old[dataset$days_elapsed_old<1] <- 0
dataset[ dataset == "na" ] <- NA

#Factor like columns
dataset$job=as.integer(as.factor(dataset$job))
dataset$marital=as.integer(as.factor(dataset$marital))
dataset$education=as.integer(as.factor(dataset$education))
dataset$device=as.integer(as.factor(dataset$device))
dataset$outcome_old=as.integer(as.factor(dataset$outcome_old))
dataset[is.na(dataset)] <- 0

# Encoding the target feature as factor
dataset$y = factor(dataset$y, levels = c(0, 1))

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$y, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling #no need to scale,but to visualise in high resolution if we scale, the results will be fast otherwise code may break
training_set[-17] = scale(training_set[-17])
test_set[-17] = scale(test_set[-17])

# Fitting Decision TreeClassification to the Training set
library(rpart)
classifier = rpart(formula = y ~ .,
                   data = training_set)

# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-17], type = 'class') 

# Making the Confusion Matrix
cm = table(test_set[, 17], y_pred)

print("=====================================Decision Trees=====================================")

library(ggplot2)
library(lattice)
library(caret)
confusionMatrix(cm)

#=================================================================================================================
# k-nearest neighbors (K-NN)

rm(list=ls())

# Importing the dataset
dataset = read.csv('train.csv')

dataset$days_elapsed_old[dataset$days_elapsed_old<1] <- 0
dataset[ dataset == "na" ] <- NA

#Factor like columns
dataset$job=as.integer(as.factor(dataset$job))
dataset$marital=as.integer(as.factor(dataset$marital))
dataset$education=as.integer(as.factor(dataset$education))
dataset$device=as.integer(as.factor(dataset$device))
dataset$outcome_old=as.integer(as.factor(dataset$outcome_old))
dataset[is.na(dataset)] <- 0

# Encoding the target feature as factor
# Encoding the target feature as factor #(the values are considered as numeric values i.e 1 > 0 but we don't want that. 
#Instead we want them to consider as factors i.e 1 and 0 as two different categories.)
dataset$y = factor(dataset$y, levels = c(0, 1))

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$y, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling
training_set[-17] = scale(training_set[-17])
test_set[-17] = scale(test_set[-17])

# Fitting K-NN to the Training set and predicting the test set results
#install.packages('class')
library(class)
y_pred = knn(train = training_set[, -17],
             test = test_set[, -17],
             cl = training_set[, 17],k = 20)

# Making the Confusion Matrix
cm = table(test_set[, 17], y_pred)
print("=====================================KNN=====================================")
library(ggplot2)
library(lattice)
library(caret)
confusionMatrix(cm)
```


\begin{table} 
\begin{center}
\begin{tabular}{|l|c|c|c|} \hline
& Training error & CV error & Public Leaderboard error (if available)\\
  kNN & ... & ... & ...\\
  Ridge & ... & ... & ...\\
  lasso & ... & ... & ...\\
  ElasticNet & ... & ... & ... \\
  random forest & ... & ... \\
  SVM & ... & ... \\
  LDA & ... & ... \\
  QDA & ... & ... \\
  C5.0 & ... & ... \\
  
\hline
\end{tabular}
\end{center}
\caption{Training and CV error of the different models.} \label{tab_res}
\end{table}

\section{Some general comments}
Here are some comments that apply to many of the intermediate reports.
\begin{itemize}
\item
Write your team name on the front page.
\item
Write you own original report, do not follow step by step the exercise but rather think about a useful structure for your own report.
\item 
Use plots and figures, but only those that contain relevant information.
\item
In R plots, make sure that there are labels on each axis, that they are readable, that there is a caption that says what the plot or figure shows, the size of the points/lines is appropriate, the form of the plot is as you want it (e.g., squared), etc.
\item
Use tables, again be careful to describe what the table shows in a caption.
\item 
Avoid lengthy R output of model fits.
\item
Explain what you do: if you use a model, give a brief description in terms of a formula. You don't have to reproduce what we did in lecture, but the 
report should be readable on its own and be consistently written and structured.
\item
In the same vein as the previous comment, if you use notation like AIC, explain what this is and why you use it.
\item
The response is not required to be normal in a linear model (or any other method), it is only assumed to be normal conditional on the predictor values, that is, the residuals should be normal. Slight violation of this does not mean that the model is useless for prediction. Test its performance with CV.
\item
Careful with excluding predictors, even if there is high corrlation, there might be additional information. Test with CV.
\item
It is always good to know the difficulty of the problem. One can assess this by starting with simple benchmark models like the overall mean, kNN, LM, etc. In the report you should try these models and report their errors, this gives you also a feeling about what type of model performs well on this data. You don't have
to report the errors 20 different LMs with interactions, just choose the relevant models (e.g., the best LM with interactions).
\item
If you use CV (and you should), describe what you do exactly. Make sure to use the same CV for all models to make them comparable.
\item
Try to use Latex or RMarkdown, it looks better!
\end{itemize}


\section{Some tests}
Hello test




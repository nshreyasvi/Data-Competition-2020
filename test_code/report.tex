% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Machine learning Data Competition 2020},
  pdfauthor={Shreyasvi Natraj},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\title{Machine learning Data Competition 2020}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Report I.}
\author{Shreyasvi Natraj}
\date{}

\begin{document}
\maketitle

\section{Introduction}

For the given data competition, we were provided with data pertaining to
previous adverstisement campaigns as well as demographics of users who
have been a part of the survey conducted.

For the objective of the given task, we are required to train a model
that can be used in order to predict whether if a user is likely to have
a ``conversion'' where a ``conversion'' refers to the user clicking on
the advertisement and subscribing to the service.

Since the data provided is used in order to predict a categorical
variable i.e.~``conversion/y'', we planned to use do a quick an dirty
implementation of the following models to check for their accuracy:

\emph{K-Nearest Neighbours }Random Forest \emph{LDA, QDA \& C5.0
}Supported Vector Machines *Logistic Regression

We initially carried out with exploratory data analysis for the data
provided to us by considering na values as NaN values.

\section{Exploratory data analysis}

We observed from this that it would not be a good idea to not consider
the na values as NaN but as a separate level. However, based on
similarity in between the classes, we can merge different levels of a
factor into lesser number of levels so they are easier for our model to
interpret.

\subsection{Interpretation}

We also carried out the same process of data analysis after converting
categorical variables into inteager format which tend to show a similar
fashion to the current analysis being carried out. However, we replaced
``na'' values with 0 which made the data much more consistent. We also
observed that \texttt{time\_spent},\texttt{outcome\_old} and \texttt{X3}
tends to hold a very high significance when predicting conversion
\texttt{y}.

\subsection{Plots}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#install.packages("DataExplorer")}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(DataExplorer)}
\NormalTok{dataset =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'train.csv'}\NormalTok{)}
\NormalTok{dataset[ dataset }\OperatorTok{==}\StringTok{ "na"}\NormalTok{ ] <-}\StringTok{ }\OtherTok{NA}

\NormalTok{web<-}\StringTok{ }\NormalTok{dataset}
\KeywordTok{glimpse}\NormalTok{(web)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 8,526
## Columns: 17
## $ age              <int> 35, 42, 38, 71, 37, 26, 27, 28, 57, 44, 34, 26, 33...
## $ job              <fct> manager, manager, industrial_worker, retired, unem...
## $ marital          <fct> single, married, married, married, single, married...
## $ education        <fct> grad_school, grad_school, university, high_school,...
## $ device           <fct> NA, smartphone, NA, smartphone, desktop, smartphon...
## $ day              <int> 30, 17, 14, 13, 27, 17, 30, 7, 26, 28, 12, 2, 6, 2...
## $ month            <int> 5, 7, 5, 11, 4, 7, 6, 5, 5, 12, 8, 12, 5, 5, 11, 2...
## $ time_spent       <dbl> 37.65, 39.25, 10.50, 8.80, 20.80, 57.00, 3.75, 10....
## $ banner_views     <int> 1, 1, 2, 2, 2, 3, 5, 1, 5, 1, 1, 1, 1, 2, 1, 2, 1,...
## $ banner_views_old <int> 0, 0, 0, 1, 3, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 5,...
## $ days_elapsed_old <int> -1, -1, -1, 98, 179, -1, -1, 339, -1, -1, -1, -1, ...
## $ outcome_old      <fct> NA, NA, NA, success, other, NA, NA, other, NA, NA,...
## $ X1               <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,...
## $ X2               <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...
## $ X3               <int> 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,...
## $ X4               <dbl> 0.07793293, 0.07283969, 0.07607176, 0.09640840, 0....
## $ y                <int> 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#To go with glimpse(), DataExplorer itself has got a function called introduce()}
\KeywordTok{introduce}\NormalTok{(web)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   rows columns discrete_columns continuous_columns all_missing_columns
## 1 8526      17                5                 12                   0
##   total_missing_values complete_rows total_observations memory_usage
## 1                 8911          1917             144942       655976
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#The same introduce() could also be plotted in a pretty graph.}
\KeywordTok{plot_intro}\NormalTok{(web,  }\DataTypeTok{ggtheme =} \KeywordTok{theme_dark}\NormalTok{(),}
             \DataTypeTok{title =} \StringTok{"EDA with Data Explorer"}\NormalTok{,}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Data Distribution-1} 

}

\caption{Data Distribution}\label{fig:Data Distribution}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot_missing}\NormalTok{(web,  }
             \DataTypeTok{ggtheme =} \KeywordTok{theme_dark}\NormalTok{(),}
             \DataTypeTok{title =} \StringTok{" Features missing from the whole observations"}\NormalTok{,}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Missing variables-1} 

}

\caption{Missing Columns}\label{fig:Missing variables}
\end{figure}

\#\#\#EDA for Continuous variables

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##for univariate}

\NormalTok{DataExplorer}\OperatorTok{::}\KeywordTok{plot_histogram}\NormalTok{(web,  }
             \DataTypeTok{ggtheme =} \KeywordTok{theme_dark}\NormalTok{(),}
             \DataTypeTok{title =} \StringTok{" Histogram of continuous features"}\NormalTok{,}
\NormalTok{             )}

\KeywordTok{plot_density}\NormalTok{(web,  }
             \DataTypeTok{ggtheme =} \KeywordTok{theme_dark}\NormalTok{(),}
             \DataTypeTok{title =} \StringTok{" Density of continuous features"}\NormalTok{,}
\NormalTok{             )  }\CommentTok{# age, time_spent, X4 are right skewed}
\CommentTok{#outcome old hase a mode which is na-> remove this category? or remove this variable??}

\KeywordTok{plot_bar}\NormalTok{(web,  }
             \DataTypeTok{ggtheme =} \KeywordTok{theme_dark}\NormalTok{(),}
             \DataTypeTok{title =} \StringTok{" Density of continuous features"}\NormalTok{,}
\NormalTok{             )  }\CommentTok{##VISUALIZE DATA WHEN X2=0 AND =1 (subsetting)}

                    
\NormalTok{                    a<-}\StringTok{ }\KeywordTok{filter}\NormalTok{(web, web}\OperatorTok{$}\NormalTok{X1}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{                    b<-}\StringTok{ }\KeywordTok{filter}\NormalTok{(web, web}\OperatorTok{$}\NormalTok{X1}\OperatorTok{==}\DecValTok{1}\NormalTok{)}
                    \KeywordTok{plot_bar}\NormalTok{(a)}
                    \KeywordTok{plot_bar}\NormalTok{(b)}
                    \KeywordTok{plot_density}\NormalTok{(a,}
                                 \DataTypeTok{title =} \StringTok{" a"}\NormalTok{)}
                    \KeywordTok{plot_density}\NormalTok{(b,}
                                 \DataTypeTok{title =} \StringTok{" b"}\NormalTok{) }

\CommentTok{##for bivariate}
  \KeywordTok{plot_boxplot}\NormalTok{(web, }\DataTypeTok{by=} \StringTok{'day'}\NormalTok{ , }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{,   }
             \DataTypeTok{ggtheme =} \KeywordTok{theme_dark}\NormalTok{(),}
             \DataTypeTok{title =} \StringTok{" Boxplot of continuous features by day"}\NormalTok{,}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-1} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-2} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-3} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-4} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-5} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-6} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-7} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-8} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-9} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Continous variables-10} 

}

\caption{Continous Variables}\label{fig:Continous variables}
\end{figure}

\#\#\#Correlation Plot

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##autocorr plot}
\KeywordTok{plot_correlation}\NormalTok{(web, }\DataTypeTok{cor_args =} \KeywordTok{list}\NormalTok{( }\StringTok{'use'}\NormalTok{ =}\StringTok{ 'complete.obs'}\NormalTok{),  }
             \DataTypeTok{ggtheme =} \KeywordTok{theme_dark}\NormalTok{(),}
             \DataTypeTok{title =} \StringTok{" Autocorr Plot"}\NormalTok{,}
\NormalTok{             )}

\CommentTok{##continurous correlation plot}
\KeywordTok{plot_correlation}\NormalTok{(web, }\DataTypeTok{type =} \StringTok{'c'}\NormalTok{,}\DataTypeTok{cor_args =} \KeywordTok{list}\NormalTok{( }\StringTok{'use'}\NormalTok{ =}\StringTok{ 'complete.obs'}\NormalTok{),  }
             \DataTypeTok{ggtheme =} \KeywordTok{theme_dark}\NormalTok{(),}
             \DataTypeTok{title =} \StringTok{" Continuous corr Plot"}\NormalTok{,}
\NormalTok{             )  }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Correlation Plot-1} \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Correlation Plot-2} 

}

\caption{Correlation Plot}\label{fig:Correlation Plot}
\end{figure}

\#\#\#EDA for Categorical`

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot_bar}\NormalTok{(a, }\DataTypeTok{maxcat =} \DecValTok{390}\NormalTok{, }\DataTypeTok{parallel =} \OtherTok{FALSE}\NormalTok{,  }
             \DataTypeTok{ggtheme =} \KeywordTok{theme_dark}\NormalTok{(),}
             \DataTypeTok{title =} \StringTok{" Categorical Features Plot"}\NormalTok{,}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{report_files/figure-latex/Categorical Features Plot-1} 

}

\caption{Categorical Features Plot}\label{fig:Categorical Features Plot}
\end{figure}

For more sophisticated graphs, that span over multiple pages, see
function \texttt{ggarrange()} from \texttt{ggpubr} package (see
\href{http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/81-ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page/}{link}).

For good-looking colors, have a look at the Paul Tol's palette
\url{https://personal.sron.nl/~pault/}.

\subsection{Tables}

To display a table, look at the \texttt{kable()} function from
\texttt{knitr} package. Also, consider the \texttt{kableExtra} package
for more sophisticated options (see
\href{https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf}{link}).
In Table \ref{tab:tblname}, we show an example that uses both
\texttt{kable} and \texttt{kableExtra}.

You can reference a table by putting the code
\texttt{\textbackslash{}\textbackslash{}label\{tab:tblname\}} inside the
caption. See code below. Then, you see that the reference works (see
Table \ref{tab:tblname}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Prepare data to put in the table}
\NormalTok{dat2 <-}\StringTok{ }\NormalTok{mtcars }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(cyl) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{Average =} \KeywordTok{mean}\NormalTok{(mpg), }\DataTypeTok{Max =} \KeywordTok{max}\NormalTok{(mpg), }\DataTypeTok{Sqrt =} \KeywordTok{sum}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(mpg)))}

\CommentTok{# Print table}
\NormalTok{capt <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{label\{tab:tblname\}Average and "}\NormalTok{,}
              \StringTok{"maximum miles per gallon for each number of cylindyers class."}\NormalTok{)}
\KeywordTok{kable}\NormalTok{(dat2,}
      \DataTypeTok{format =} \StringTok{"latex"}\NormalTok{,}
      \DataTypeTok{longtable =}\NormalTok{ F,}
      \DataTypeTok{booktabs =}\NormalTok{ T,}
      \DataTypeTok{digits =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{caption =}\NormalTok{ capt) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options =} \KeywordTok{c}\NormalTok{(}\StringTok{"striped"}\NormalTok{, }\StringTok{"hold_position"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:table1}\label{tab:tblname}Average and  maximum miles per gallon for each number of cylindyers class.}
\centering
\begin{tabular}[t]{rrrr}
\toprule
cyl & Average & Max & Sqrt\\
\midrule
\rowcolor{gray!6}  4 & 26.66 & 33.9 & 56.62\\
6 & 19.74 & 21.4 & 31.08\\
\rowcolor{gray!6}  8 & 15.10 & 19.2 & 54.21\\
\bottomrule
\end{tabular}
\end{table}

If you want to manually insert the values in the table, you can do it,
too (see Table \ref{tab:tab2}).

\begin{table}[H]
\caption{\label{tab:tab2}Number of different levels and the number of predictors that have this amount of levels.}
\centering
\begin{tabular}{lcccc}
\toprule
 & Col 1 & Col 2 & Col 3 & Col 4\\
\midrule
\rowcolor{gray!6}
Number of different values & 2 & 4 & 12 & $> 300$\\
Number of predictors & ... & ... & ... & ...\\
\bottomrule
\end{tabular}
\end{table}

\section{Models}

\subsection{Linear model}

The first approach is to fit a linear model, that is, the regression
function \(f\) in \eqref{eq1} is assumed to be of the form \[
  f(X) = \beta_0 + \sum_{j=1}^p \beta_j X_j,
\] where \(\beta_j\in \mathbb R\) is the coefficient of the \(j\)th
predictor.

Fitting this model to the training data, we obtain a predictive model
\(\hat f_{LM}\). The training and cross-validation error of this model
can be found in Table \ref{tab_res}\ldots.

There are \ldots{} statistically significant predictors\ldots{}

Possible meaning and interpretation of some predictors

\section{Validation}

We implement a cross-validation \ldots.

\section{Results}

\subsection{Preliminary Implementation}

We started by dividing the set into 75-35 percent split and running them
through different machine learning models in a crude manner to check out
of the box which model tends to perform best on the given dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Support Vector Machine}
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}
\CommentTok{# Importing the dataset}
\NormalTok{dataset =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'train.csv'}\NormalTok{)}

\NormalTok{dataset}\OperatorTok{$}\NormalTok{days_elapsed_old[dataset}\OperatorTok{$}\NormalTok{days_elapsed_old}\OperatorTok{<}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
\NormalTok{dataset[ dataset }\OperatorTok{==}\StringTok{ "na"}\NormalTok{ ] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{#Factor like columns}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{job=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{job))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{marital=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{marital))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{education=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{education))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{device=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{device))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{outcome_old=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{outcome_old))}
\NormalTok{dataset[}\KeywordTok{is.na}\NormalTok{(dataset)] <-}\StringTok{ }\DecValTok{0}

\CommentTok{# Encoding the target feature as factor}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{y=}\StringTok{ }\KeywordTok{factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{# Splitting the dataset into the Training set and Test set}
\CommentTok{#install.packages('caTools')}
\KeywordTok{library}\NormalTok{(caTools)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{split =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{SplitRatio =} \FloatTok{0.75}\NormalTok{)}

\NormalTok{training_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE}\NormalTok{)}
\NormalTok{test_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Feature Scaling}
\NormalTok{training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}
\NormalTok{test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}

\CommentTok{# Fitting SVM to the Training set}
\CommentTok{#install.packages('e1071')}
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{classifier =}\StringTok{ }\KeywordTok{svm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ y }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                 \DataTypeTok{data =}\NormalTok{ training_set,}
                 \DataTypeTok{type =} \StringTok{'C-classification'}\NormalTok{,}
                 \DataTypeTok{kernel =} \StringTok{'radial'}\NormalTok{)}

\CommentTok{# Predicting the Test set results}
\NormalTok{y_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{],}\DataTypeTok{drop=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{y_pred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    2    5    6    7   11   15   16   18   20   21   31   36   37   41   42   43 
##    1    1    1    0    0    0    1    0    1    0    1    1    1    0    0    1 
##   56   59   63   64   67   69   83   86   89  101  110  115  124  125  128  140 
##    1    0    1    0    1    0    0    0    0    1    0    0    1    0    1    0 
##  143  148  149  151  154  155  158  160  163  164  165  167  169  177  179  182 
##    0    0    1    0    1    0    1    0    1    1    0    0    1    0    1    0 
##  183  187  194  197  201  202  205  211  217  218  220  225  226  229  236  245 
##    0    0    0    0    0    0    0    1    1    1    0    1    0    0    1    0 
##  247  252  254  256  261  264  267  270  274  279  283  290  291  293  298  301 
##    0    0    1    1    0    0    0    1    0    1    0    0    0    0    0    0 
##  303  314  321  322  323  324  326  334  339  341  346  349  350  351  353  365 
##    1    1    1    1    0    1    0    1    0    0    0    1    0    0    0    0 
##  366  367  371  374  383  385  389  397  399  404  405  406  407  413  414  420 
##    0    0    0    0    0    1    1    0    1    0    1    1    0    0    1    1 
##  421  422  438  439  445  446  449  450  456  464  468  474  476  481  485  499 
##    1    0    1    1    0    0    1    0    1    1    1    1    0    1    1    0 
##  500  506  509  512  513  520  524  528  531  532  535  537  540  551  552  554 
##    0    1    1    0    1    0    0    0    1    0    1    0    0    1    0    1 
##  555  559  560  570  573  574  587  589  598  599  603  604  605  607  608  616 
##    0    0    0    0    1    1    0    1    1    1    1    0    0    1    1    1 
##  622  626  633  634  637  638  649  654  655  656  657  662  666  667  668  669 
##    1    1    0    0    1    1    0    0    0    0    0    0    1    0    1    1 
##  673  674  675  676  682  683  685  689  692  705  711  717  719  721  725  726 
##    0    1    0    1    1    0    0    0    0    0    0    0    1    1    0    1 
##  727  729  731  734  737  741  745  751  756  762  766  768  774  775  778  780 
##    0    1    0    0    0    0    1    0    1    0    0    0    0    1    1    1 
##  784  788  794  796  797  802  803  808  816  818  821  822  827  829  832  838 
##    0    0    0    0    1    0    0    0    1    1    0    0    0    0    1    0 
##  842  848  849  860  861  865  870  876  878  880  881  887  888  892  893  900 
##    1    0    0    1    0    1    1    0    0    0    0    0    1    1    0    0 
##  901  907  913  915  919  922  923  925  927  931  936  944  951  953  954  958 
##    1    0    0    0    1    0    0    0    0    0    1    1    0    0    0    1 
##  959  960  962  964  967  971  982  998  999 1003 1008 1013 1026 1028 1034 1042 
##    0    1    0    1    1    0    0    0    0    1    0    1    1    0    1    0 
## 1047 1049 1050 1056 1062 1069 1070 1081 1085 1089 1093 1096 1111 1119 1129 1131 
##    0    1    1    1    0    0    0    0    0    0    1    1    1    0    1    1 
## 1133 1134 1138 1140 1142 1146 1147 1148 1153 1156 1162 1167 1170 1175 1176 1182 
##    0    0    1    1    0    0    0    0    0    0    1    1    0    0    0    0 
## 1185 1193 1198 1202 1213 1214 1225 1232 1240 1241 1244 1249 1252 1257 1262 1266 
##    0    1    0    1    1    1    0    1    0    0    0    0    1    1    0    0 
## 1269 1270 1278 1284 1287 1290 1294 1303 1305 1313 1318 1319 1329 1330 1335 1338 
##    0    1    0    0    1    1    1    0    1    1    1    0    1    0    0    1 
## 1339 1341 1344 1347 1349 1356 1357 1364 1367 1368 1373 1374 1375 1379 1383 1385 
##    0    1    1    0    1    0    0    0    1    1    0    0    0    0    0    0 
## 1389 1396 1404 1406 1407 1408 1410 1412 1419 1421 1423 1427 1428 1429 1432 1436 
##    1    1    0    1    0    1    0    1    0    1    0    1    0    1    0    1 
## 1443 1446 1451 1453 1457 1463 1466 1473 1475 1476 1477 1478 1479 1481 1483 1484 
##    0    0    1    1    0    0    1    0    1    1    0    0    0    1    1    1 
## 1485 1487 1492 1496 1499 1501 1507 1508 1512 1516 1520 1523 1527 1529 1533 1546 
##    0    0    1    0    0    1    0    0    0    1    1    1    1    0    1    0 
## 1547 1548 1552 1553 1558 1561 1565 1569 1571 1577 1580 1581 1599 1619 1621 1623 
##    0    1    0    0    0    1    1    1    1    0    1    1    1    0    1    0 
## 1634 1638 1640 1641 1645 1652 1665 1673 1677 1683 1689 1694 1718 1719 1723 1727 
##    1    1    0    1    0    0    1    1    1    0    0    0    1    0    0    0 
## 1729 1730 1731 1735 1736 1737 1740 1743 1746 1751 1752 1754 1760 1761 1763 1767 
##    1    0    0    1    0    1    1    0    1    1    0    0    0    1    1    1 
## 1769 1782 1783 1792 1796 1803 1806 1807 1808 1816 1821 1823 1824 1827 1830 1837 
##    0    0    0    1    0    0    1    1    0    0    0    0    1    0    0    0 
## 1839 1840 1843 1846 1849 1855 1856 1858 1864 1866 1870 1872 1876 1878 1882 1891 
##    0    0    0    0    1    0    0    0    0    0    0    1    1    0    1    0 
## 1893 1900 1904 1925 1926 1928 1929 1933 1939 1943 1956 1964 1966 1972 1975 1976 
##    1    1    1    0    1    0    0    1    1    0    1    0    0    1    0    0 
## 1981 1991 1999 2003 2004 2007 2008 2015 2017 2018 2020 2027 2028 2029 2035 2042 
##    0    1    1    1    0    0    0    0    0    0    0    0    1    0    1    0 
## 2046 2049 2050 2052 2053 2056 2073 2074 2076 2081 2083 2084 2088 2093 2098 2102 
##    1    0    0    0    0    0    0    0    1    1    1    0    0    0    0    0 
## 2107 2113 2115 2117 2120 2134 2135 2139 2140 2145 2150 2152 2154 2155 2162 2167 
##    1    1    0    0    0    0    1    0    1    1    0    0    1    1    0    1 
## 2168 2174 2175 2188 2198 2199 2203 2216 2217 2218 2222 2226 2230 2235 2241 2247 
##    0    1    1    0    0    1    0    1    1    0    0    1    1    0    1    0 
## 2253 2254 2255 2256 2259 2264 2270 2272 2275 2277 2278 2282 2283 2289 2309 2315 
##    1    1    0    0    1    0    0    1    0    0    0    0    0    1    1    0 
## 2316 2322 2327 2329 2330 2332 2338 2345 2349 2353 2356 2357 2365 2373 2376 2377 
##    1    0    0    0    0    0    1    1    0    0    1    0    0    0    0    0 
## 2385 2389 2391 2392 2394 2400 2403 2406 2408 2412 2421 2426 2430 2431 2432 2436 
##    1    0    1    1    0    0    0    0    1    0    1    0    1    1    1    1 
## 2439 2447 2457 2460 2462 2464 2465 2466 2469 2474 2475 2479 2486 2488 2491 2504 
##    0    0    1    1    1    0    0    0    0    0    1    1    1    0    1    0 
## 2505 2510 2514 2515 2516 2520 2521 2523 2526 2531 2534 2535 2537 2538 2541 2542 
##    0    1    0    1    0    0    0    0    0    0    1    1    0    0    1    1 
## 2546 2554 2563 2574 2579 2580 2581 2582 2586 2591 2594 2595 2598 2601 2610 2615 
##    0    0    1    1    1    0    0    0    0    0    1    1    1    0    0    0 
## 2619 2626 2627 2631 2632 2639 2640 2644 2645 2647 2649 2656 2657 2658 2659 2661 
##    1    0    0    0    1    0    0    0    1    0    1    0    1    0    0    1 
## 2662 2669 2672 2675 2679 2688 2693 2695 2696 2699 2703 2704 2712 2722 2736 2744 
##    1    0    0    1    0    0    0    1    0    1    1    1    1    0    0    1 
## 2746 2747 2751 2753 2754 2755 2758 2763 2767 2768 2769 2770 2771 2780 2784 2785 
##    1    1    0    0    0    1    0    0    0    1    0    0    0    1    0    1 
## 2787 2793 2798 2803 2804 2807 2810 2812 2820 2825 2832 2834 2839 2843 2848 2850 
##    0    0    1    1    1    0    0    0    0    0    0    1    0    1    0    1 
## 2856 2866 2872 2873 2878 2879 2883 2892 2899 2900 2902 2905 2906 2915 2916 2923 
##    0    0    0    0    1    1    1    1    1    0    0    1    0    1    0    0 
## 2927 2934 2936 2940 2943 2945 2949 2951 2957 2959 2962 2963 2965 2967 2981 2984 
##    1    1    0    1    1    0    0    1    0    0    1    0    0    1    1    0 
## 2985 2989 3004 3011 3017 3028 3037 3042 3044 3050 3058 3059 3061 3064 3067 3068 
##    0    1    1    0    1    1    1    0    1    0    0    0    0    0    0    1 
## 3071 3074 3075 3076 3082 3089 3093 3095 3098 3101 3102 3105 3106 3111 3119 3127 
##    1    0    0    1    1    0    0    0    0    0    1    1    1    0    1    0 
## 3134 3136 3137 3147 3151 3160 3172 3173 3180 3181 3185 3186 3189 3193 3196 3198 
##    0    0    1    0    1    0    1    0    0    1    0    0    0    0    0    1 
## 3204 3206 3210 3212 3217 3227 3231 3233 3238 3240 3243 3245 3246 3249 3254 3257 
##    1    1    0    1    0    0    0    0    1    1    0    0    0    1    0    0 
## 3259 3265 3272 3282 3285 3289 3294 3305 3310 3323 3325 3328 3329 3330 3339 3341 
##    1    0    1    0    1    1    1    0    0    0    0    0    0    1    1    0 
## 3344 3348 3350 3354 3356 3357 3359 3363 3364 3374 3384 3387 3395 3399 3402 3405 
##    0    0    1    0    0    1    0    0    1    1    0    0    0    0    0    1 
## 3408 3409 3416 3417 3419 3427 3430 3431 3432 3434 3435 3438 3441 3450 3461 3462 
##    0    0    0    1    0    1    0    0    0    0    0    1    1    0    1    1 
## 3464 3466 3467 3477 3478 3483 3490 3493 3499 3503 3506 3510 3514 3523 3524 3533 
##    1    1    1    0    0    1    0    0    0    1    0    0    0    0    0    1 
## 3534 3541 3542 3544 3545 3548 3549 3551 3558 3559 3563 3579 3580 3583 3585 3587 
##    0    1    0    0    1    0    1    1    1    0    0    1    1    1    1    1 
## 3590 3591 3592 3596 3597 3598 3602 3603 3608 3610 3613 3620 3621 3624 3626 3630 
##    1    0    1    1    0    0    0    1    1    1    1    0    0    0    0    1 
## 3631 3641 3646 3647 3650 3654 3659 3660 3664 3676 3689 3692 3695 3700 3706 3707 
##    0    0    0    0    0    1    1    1    0    0    1    0    1    1    0    1 
## 3711 3714 3715 3724 3726 3727 3741 3742 3749 3755 3756 3758 3761 3763 3766 3775 
##    1    0    0    0    1    1    1    0    0    1    0    0    0    0    1    0 
## 3778 3779 3781 3782 3789 3795 3801 3808 3819 3823 3824 3833 3836 3839 3841 3846 
##    1    1    1    1    0    1    0    0    0    0    1    0    0    1    0    1 
## 3847 3851 3872 3877 3878 3881 3882 3883 3893 3895 3896 3898 3899 3900 3906 3914 
##    0    0    1    1    1    0    1    0    1    0    0    0    0    0    0    0 
## 3917 3924 3927 3928 3937 3938 3941 3949 3951 3952 3958 3960 3962 3963 3965 3967 
##    0    1    0    1    1    0    0    0    1    1    0    0    0    1    1    0 
## 3969 3970 3972 3973 3974 3981 3982 3993 3996 3997 4002 4004 4006 4007 4015 4017 
##    0    1    0    1    0    1    0    0    0    0    1    0    1    0    1    0 
## 4018 4021 4033 4034 4047 4048 4056 4060 4065 4066 4069 4075 4076 4077 4078 4083 
##    1    0    1    0    0    0    0    0    0    1    0    0    0    0    0    0 
## 4088 4090 4092 4095 4097 4100 4108 4109 4110 4112 4113 4122 4127 4131 4134 4135 
##    0    1    1    0    0    1    0    0    1    1    1    0    1    1    0    0 
## 4136 4138 4139 4140 4150 4151 4152 4157 4158 4162 4166 4167 4169 4171 4175 4178 
##    1    0    0    0    0    0    0    0    0    0    0    0    1    1    1    0 
## 4184 4185 4190 4191 4197 4204 4205 4208 4209 4210 4212 4215 4217 4218 4219 4225 
##    0    0    0    1    0    1    0    1    1    0    0    0    0    1    1    1 
## 4236 4238 4241 4243 4245 4247 4248 4249 4256 4259 4266 4269 4271 4280 4282 4283 
##    1    1    1    0    1    1    0    0    0    1    1    0    1    0    1    0 
## 4287 4290 4291 4293 4317 4326 4329 4333 4334 4337 4340 4345 4353 4359 4364 4366 
##    0    1    0    0    0    0    0    1    0    0    0    0    1    0    0    1 
## 4371 4374 4377 4378 4379 4381 4388 4390 4391 4396 4403 4407 4409 4413 4415 4417 
##    0    1    0    0    1    0    1    0    0    0    0    0    1    0    0    0 
## 4419 4423 4424 4427 4428 4431 4433 4438 4439 4440 4444 4445 4448 4456 4461 4464 
##    1    1    1    0    0    0    0    0    0    0    1    1    0    0    1    0 
## 4469 4479 4481 4482 4484 4485 4486 4489 4492 4498 4502 4505 4507 4509 4510 4512 
##    0    0    0    0    1    0    0    0    0    0    1    0    0    0    1    0 
## 4517 4520 4523 4524 4526 4533 4534 4540 4542 4548 4552 4560 4572 4578 4582 4585 
##    1    0    1    0    0    0    1    1    1    0    0    1    1    1    1    0 
## 4596 4625 4630 4633 4642 4654 4656 4670 4674 4678 4688 4693 4698 4703 4715 4716 
##    1    0    1    1    1    0    0    0    1    1    1    0    1    1    0    1 
## 4722 4723 4731 4739 4742 4746 4748 4753 4757 4759 4763 4771 4775 4778 4780 4783 
##    0    1    0    0    1    0    1    1    0    1    0    1    0    0    0    0 
## 4784 4786 4787 4788 4801 4806 4807 4811 4817 4826 4831 4840 4845 4854 4858 4863 
##    1    1    0    0    1    1    0    1    0    0    0    1    1    0    1    1 
## 4868 4870 4871 4877 4879 4889 4890 4891 4899 4902 4905 4906 4908 4913 4917 4919 
##    0    0    1    1    0    0    0    0    1    0    0    0    0    0    0    0 
## 4924 4925 4926 4927 4935 4936 4937 4938 4939 4940 4946 4947 4950 4959 4965 4966 
##    1    0    0    0    0    1    1    1    0    1    0    0    0    0    0    1 
## 4968 4970 4972 4978 4980 4984 4987 4988 4989 4991 5000 5003 5006 5009 5011 5014 
##    0    0    0    0    0    1    0    0    1    0    0    0    0    0    1    0 
## 5016 5017 5020 5021 5029 5039 5041 5051 5056 5059 5064 5070 5074 5075 5087 5089 
##    1    0    1    0    1    0    0    0    1    1    0    0    1    1    0    0 
## 5093 5095 5100 5101 5116 5123 5124 5127 5131 5133 5134 5137 5138 5140 5144 5153 
##    0    0    1    1    1    1    1    0    1    1    0    1    1    1    0    1 
## 5154 5163 5166 5172 5186 5188 5189 5193 5194 5197 5204 5206 5211 5214 5215 5216 
##    1    0    0    0    1    0    0    1    0    0    0    1    1    0    0    0 
## 5218 5226 5227 5230 5236 5244 5245 5248 5250 5251 5254 5261 5262 5264 5265 5268 
##    1    0    1    1    1    1    1    0    0    1    0    0    1    1    1    0 
## 5269 5273 5275 5276 5280 5283 5284 5294 5295 5296 5300 5303 5305 5310 5330 5335 
##    0    0    0    0    1    1    1    0    1    1    0    0    1    1    0    0 
## 5352 5353 5355 5357 5358 5359 5360 5363 5365 5366 5369 5371 5376 5378 5382 5385 
##    0    0    1    0    1    1    0    0    1    0    0    0    0    1    0    1 
## 5389 5393 5400 5402 5405 5406 5407 5410 5411 5417 5418 5419 5431 5432 5433 5442 
##    1    1    1    0    1    0    1    0    0    0    0    1    0    1    1    1 
## 5449 5452 5454 5455 5462 5466 5472 5473 5474 5483 5491 5495 5503 5508 5510 5511 
##    0    0    1    0    0    0    1    1    1    1    0    1    1    0    1    1 
## 5513 5515 5520 5524 5529 5539 5545 5550 5551 5554 5566 5584 5589 5599 5602 5604 
##    0    0    1    1    1    0    0    0    0    1    0    0    0    0    1    0 
## 5607 5608 5614 5616 5621 5622 5628 5638 5642 5651 5654 5662 5666 5675 5676 5691 
##    1    1    0    0    1    1    0    1    1    0    0    0    0    0    1    0 
## 5693 5704 5707 5709 5710 5715 5718 5720 5723 5726 5729 5732 5736 5739 5740 5745 
##    1    0    0    1    0    1    0    1    0    1    0    1    0    0    0    1 
## 5746 5751 5752 5757 5758 5759 5765 5770 5772 5777 5779 5780 5782 5797 5806 5810 
##    1    0    0    1    0    0    0    1    1    0    1    0    0    1    0    0 
## 5811 5814 5820 5828 5835 5849 5857 5862 5865 5867 5868 5871 5875 5878 5881 5882 
##    1    0    0    0    0    1    1    0    0    1    0    1    0    0    0    0 
## 5887 5893 5901 5904 5909 5911 5922 5923 5924 5925 5930 5931 5932 5934 5937 5941 
##    0    1    0    0    0    0    0    0    1    0    0    1    1    0    1    0 
## 5946 5949 5951 5953 5963 5973 5977 5978 5980 5984 5993 5995 5998 6000 6004 6008 
##    1    0    1    1    0    0    0    1    1    1    0    0    1    1    0    0 
## 6015 6016 6018 6030 6035 6044 6047 6049 6050 6052 6056 6057 6059 6063 6065 6066 
##    1    1    0    0    0    1    1    0    1    1    0    0    1    0    0    0 
## 6068 6069 6076 6096 6097 6099 6100 6103 6106 6109 6115 6116 6117 6119 6121 6125 
##    0    0    1    1    0    1    1    0    1    1    0    1    0    0    0    1 
## 6129 6142 6143 6146 6148 6158 6159 6167 6168 6169 6177 6185 6190 6193 6196 6199 
##    1    0    0    1    0    0    1    1    0    1    1    1    0    0    0    1 
## 6200 6206 6211 6219 6221 6222 6223 6224 6228 6235 6237 6242 6246 6250 6258 6264 
##    1    1    0    1    1    0    1    0    0    0    1    0    0    0    0    0 
## 6265 6268 6272 6275 6284 6285 6290 6293 6294 6296 6306 6317 6324 6325 6328 6334 
##    0    0    0    0    0    1    0    0    0    0    1    0    1    1    0    1 
## 6337 6340 6344 6348 6350 6352 6353 6354 6358 6360 6361 6362 6363 6379 6380 6391 
##    0    1    0    0    1    0    0    1    0    1    0    1    1    0    0    0 
## 6393 6397 6400 6411 6415 6418 6419 6422 6423 6430 6437 6440 6444 6451 6460 6462 
##    0    1    1    1    0    0    0    1    1    0    0    0    0    1    0    0 
## 6463 6465 6466 6467 6469 6471 6477 6478 6481 6483 6485 6486 6487 6488 6489 6490 
##    0    1    0    1    1    0    1    0    1    0    0    0    1    0    0    0 
## 6491 6492 6497 6499 6502 6508 6515 6521 6522 6527 6532 6534 6535 6538 6549 6551 
##    0    0    0    0    1    0    1    1    0    1    1    1    1    1    1    0 
## 6553 6555 6559 6561 6563 6565 6566 6569 6575 6580 6582 6586 6614 6615 6617 6618 
##    1    0    0    1    0    1    0    1    0    1    1    0    0    1    0    0 
## 6620 6632 6646 6654 6658 6661 6663 6664 6665 6673 6677 6681 6682 6686 6687 6690 
##    0    0    0    0    0    0    0    1    1    0    0    1    1    0    0    1 
## 6693 6694 6695 6697 6698 6702 6705 6717 6724 6727 6728 6730 6732 6733 6738 6740 
##    1    0    1    1    0    0    1    1    1    0    0    1    1    1    1    1 
## 6743 6751 6756 6757 6760 6763 6767 6770 6771 6777 6782 6783 6784 6788 6790 6793 
##    1    0    0    0    1    1    0    0    0    1    0    1    0    0    0    1 
## 6797 6798 6803 6807 6813 6815 6825 6832 6837 6848 6850 6852 6855 6859 6864 6868 
##    0    0    1    0    1    0    0    1    1    0    0    0    0    0    1    0 
## 6871 6873 6885 6887 6891 6895 6900 6903 6909 6920 6922 6926 6930 6932 6941 6952 
##    1    1    1    0    0    1    0    0    1    0    0    0    0    0    0    0 
## 6953 6958 6961 6964 6974 6976 6977 6978 6979 6981 6986 6993 7002 7005 7015 7020 
##    0    1    0    0    0    0    0    0    1    1    0    1    0    1    0    1 
## 7022 7026 7028 7039 7043 7047 7058 7073 7074 7081 7083 7084 7085 7086 7087 7089 
##    0    1    0    0    0    0    1    1    1    0    0    0    0    1    0    1 
## 7094 7097 7098 7104 7110 7115 7118 7124 7129 7133 7138 7146 7155 7160 7164 7165 
##    1    0    0    0    1    1    1    0    1    1    1    0    0    0    1    0 
## 7169 7172 7173 7180 7187 7194 7196 7200 7202 7203 7204 7211 7215 7217 7220 7226 
##    1    0    0    1    0    0    0    0    1    0    0    0    1    0    0    0 
## 7237 7240 7241 7244 7248 7249 7254 7263 7264 7265 7267 7268 7271 7273 7274 7277 
##    1    1    0    0    0    1    1    1    0    0    0    0    1    0    0    0 
## 7278 7283 7289 7295 7298 7300 7311 7322 7325 7326 7327 7332 7337 7343 7344 7345 
##    1    0    0    0    1    0    0    1    1    0    0    1    1    1    0    0 
## 7347 7348 7351 7355 7357 7363 7366 7367 7368 7372 7374 7380 7383 7389 7391 7393 
##    0    0    0    1    1    0    1    0    0    0    1    0    0    0    0    0 
## 7396 7402 7405 7409 7415 7418 7425 7428 7433 7435 7438 7439 7445 7448 7457 7458 
##    1    0    1    0    1    0    0    0    1    0    1    0    1    1    0    0 
## 7463 7465 7470 7471 7475 7476 7483 7484 7487 7489 7491 7493 7494 7495 7498 7504 
##    0    1    0    1    0    0    1    0    1    0    0    1    0    0    0    1 
## 7508 7509 7521 7523 7524 7527 7538 7542 7546 7547 7553 7560 7570 7571 7578 7579 
##    0    1    1    1    0    1    0    1    1    1    0    0    0    0    0    1 
## 7583 7584 7593 7597 7603 7606 7608 7609 7615 7618 7620 7628 7630 7631 7637 7638 
##    0    0    1    1    1    0    1    0    0    0    0    1    0    0    0    0 
## 7640 7642 7644 7649 7653 7663 7668 7669 7671 7672 7673 7677 7682 7684 7686 7687 
##    0    1    1    1    0    0    0    1    0    0    1    0    1    0    0    1 
## 7691 7694 7698 7701 7702 7718 7721 7722 7724 7725 7738 7742 7744 7745 7747 7756 
##    1    0    0    0    0    0    0    0    1    0    1    1    1    1    1    1 
## 7760 7767 7768 7771 7772 7774 7783 7785 7787 7788 7793 7798 7799 7801 7805 7808 
##    0    1    0    1    1    1    0    1    1    1    0    0    0    1    1    0 
## 7813 7815 7821 7824 7829 7832 7839 7840 7845 7847 7850 7851 7865 7868 7874 7879 
##    1    0    0    0    0    0    1    1    1    0    1    0    0    0    1    0 
## 7881 7882 7884 7886 7889 7890 7893 7902 7904 7908 7912 7918 7920 7921 7925 7930 
##    1    0    0    0    1    0    0    1    1    0    1    0    0    0    1    0 
## 7931 7937 7938 7947 7948 7952 7960 7962 7970 7972 7973 7975 7978 7979 7980 7983 
##    0    0    0    1    1    0    1    1    0    0    1    0    0    1    1    1 
## 7985 7987 7988 7991 8000 8002 8015 8019 8021 8025 8033 8036 8045 8051 8052 8056 
##    0    1    1    1    0    1    1    0    0    1    1    0    1    1    0    0 
## 8057 8059 8063 8067 8078 8079 8080 8083 8087 8091 8096 8099 8104 8108 8109 8112 
##    1    0    1    0    1    0    0    0    0    0    0    0    0    0    1    0 
## 8124 8136 8138 8140 8142 8144 8147 8149 8153 8154 8157 8160 8170 8176 8183 8206 
##    0    0    0    1    0    0    0    0    1    1    1    1    0    1    1    0 
## 8207 8209 8211 8216 8230 8241 8245 8249 8255 8265 8266 8267 8273 8284 8285 8289 
##    0    1    0    0    0    0    0    0    0    0    0    1    1    1    1    0 
## 8294 8297 8303 8305 8319 8320 8322 8325 8330 8335 8339 8340 8341 8349 8357 8358 
##    0    0    0    1    1    1    1    0    1    1    0    1    1    0    1    0 
## 8366 8371 8372 8373 8374 8386 8389 8396 8399 8412 8413 8425 8429 8434 8438 8440 
##    0    0    1    0    1    0    0    0    0    0    0    0    1    1    1    1 
## 8446 8451 8453 8454 8462 8464 8469 8472 8473 8479 8481 8487 8491 8494 8499 8501 
##    0    1    0    1    0    1    0    0    0    1    1    0    0    1    1    0 
## 8504 8514 8521 8526 
##    1    0    1    0 
## Levels: 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Making the Confusion Matrix}
\NormalTok{cm =}\StringTok{ }\KeywordTok{table}\NormalTok{(test_set[, }\DecValTok{17}\NormalTok{], y_pred)}
\NormalTok{cm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    y_pred
##        0    1
##   0 1083  157
##   1  168  724
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\StringTok{"======================================SVM====================================="}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "======================================SVM====================================="
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(lattice)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{confusionMatrix}\NormalTok{(cm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##    y_pred
##        0    1
##   0 1083  157
##   1  168  724
##                                           
##                Accuracy : 0.8476          
##                  95% CI : (0.8316, 0.8626)
##     No Information Rate : 0.5868          
##     P-Value [Acc > NIR] : <2e-16          
##                                           
##                   Kappa : 0.6862          
##                                           
##  Mcnemar's Test P-Value : 0.5791          
##                                           
##             Sensitivity : 0.8657          
##             Specificity : 0.8218          
##          Pos Pred Value : 0.8734          
##          Neg Pred Value : 0.8117          
##              Prevalence : 0.5868          
##          Detection Rate : 0.5080          
##    Detection Prevalence : 0.5816          
##       Balanced Accuracy : 0.8438          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#================================================================================================================}
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}
\CommentTok{#Random Forest Classification}

\CommentTok{# Importing the dataset}
\NormalTok{dataset =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'train.csv'}\NormalTok{)}

\NormalTok{dataset}\OperatorTok{$}\NormalTok{days_elapsed_old[dataset}\OperatorTok{$}\NormalTok{days_elapsed_old}\OperatorTok{<}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
\NormalTok{dataset[ dataset }\OperatorTok{==}\StringTok{ "na"}\NormalTok{ ] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{#Factor like columns}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{job=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{job))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{marital=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{marital))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{education=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{education))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{device=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{device))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{outcome_old=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{outcome_old))}
\NormalTok{dataset[}\KeywordTok{is.na}\NormalTok{(dataset)] <-}\StringTok{ }\DecValTok{0}

\CommentTok{# Encoding the target feature as factor}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{y =}\StringTok{ }\KeywordTok{factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{# Splitting the dataset into the Training set and Test set}
\CommentTok{# install.packages('caTools')}
\KeywordTok{library}\NormalTok{(caTools)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{SplitRatio =} \FloatTok{0.75}\NormalTok{)}
\NormalTok{training_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE}\NormalTok{)}
\NormalTok{test_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Feature Scaling #for higher resolution visualisation only we are using feature scaling,RF doesnt need feature scaling}
\NormalTok{training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}
\NormalTok{test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}

\CommentTok{# Fitting Random Forest Classification to the Training set}
\CommentTok{#install.packages('randomForest')}
\KeywordTok{library}\NormalTok{(randomForest)}
\NormalTok{classifier =}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{],}
                          \DataTypeTok{y =}\NormalTok{ training_set}\OperatorTok{$}\NormalTok{y)}\CommentTok{#,                           ntree = 700)                 }

\CommentTok{# Predicting the Test set results}
\NormalTok{y_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}
\NormalTok{y_pred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    2    5    6    7   11   15   16   18   20   21   31   36   37   41   42   43 
##    1    1    1    0    0    0    0    0    1    0    0    1    1    0    0    1 
##   56   59   63   64   67   69   83   86   89  101  110  115  124  125  128  140 
##    1    0    1    0    1    0    0    0    0    1    0    0    1    1    0    0 
##  143  148  149  151  154  155  158  160  163  164  165  167  169  177  179  182 
##    0    0    1    0    1    0    1    0    1    1    0    1    1    0    0    0 
##  183  187  194  197  201  202  205  211  217  218  220  225  226  229  236  245 
##    0    0    0    0    0    1    0    1    1    1    0    1    0    0    1    0 
##  247  252  254  256  261  264  267  270  274  279  283  290  291  293  298  301 
##    0    0    1    1    0    0    0    1    0    1    0    0    0    0    0    0 
##  303  314  321  322  323  324  326  334  339  341  346  349  350  351  353  365 
##    1    1    1    1    1    1    0    1    1    0    0    1    0    0    0    0 
##  366  367  371  374  383  385  389  397  399  404  405  406  407  413  414  420 
##    0    0    0    0    1    0    1    0    1    0    1    1    0    0    1    0 
##  421  422  438  439  445  446  449  450  456  464  468  474  476  481  485  499 
##    1    0    1    1    0    0    1    0    1    1    1    1    0    1    1    0 
##  500  506  509  512  513  520  524  528  531  532  535  537  540  551  552  554 
##    0    1    1    0    0    0    0    0    1    0    1    0    0    1    0    1 
##  555  559  560  570  573  574  587  589  598  599  603  604  605  607  608  616 
##    0    0    0    0    1    1    0    1    1    1    1    0    0    1    1    1 
##  622  626  633  634  637  638  649  654  655  656  657  662  666  667  668  669 
##    1    1    1    0    1    1    0    0    0    0    0    0    1    1    1    1 
##  673  674  675  676  682  683  685  689  692  705  711  717  719  721  725  726 
##    0    1    0    1    1    0    1    0    0    0    0    0    1    1    0    1 
##  727  729  731  734  737  741  745  751  756  762  766  768  774  775  778  780 
##    0    1    0    1    0    0    1    0    1    1    0    0    0    0    0    1 
##  784  788  794  796  797  802  803  808  816  818  821  822  827  829  832  838 
##    0    0    0    0    1    0    0    0    1    0    0    0    0    0    1    0 
##  842  848  849  860  861  865  870  876  878  880  881  887  888  892  893  900 
##    1    0    0    1    0    1    1    0    0    0    0    0    0    1    0    0 
##  901  907  913  915  919  922  923  925  927  931  936  944  951  953  954  958 
##    1    0    0    0    1    0    0    0    0    0    1    0    0    0    0    1 
##  959  960  962  964  967  971  982  998  999 1003 1008 1013 1026 1028 1034 1042 
##    0    1    0    0    1    0    0    0    0    1    0    1    0    0    1    0 
## 1047 1049 1050 1056 1062 1069 1070 1081 1085 1089 1093 1096 1111 1119 1129 1131 
##    0    0    0    1    0    0    0    0    0    0    1    1    0    0    1    1 
## 1133 1134 1138 1140 1142 1146 1147 1148 1153 1156 1162 1167 1170 1175 1176 1182 
##    1    0    1    1    0    0    0    0    0    0    1    1    0    0    0    0 
## 1185 1193 1198 1202 1213 1214 1225 1232 1240 1241 1244 1249 1252 1257 1262 1266 
##    0    1    0    1    1    1    0    1    0    0    0    1    1    1    0    0 
## 1269 1270 1278 1284 1287 1290 1294 1303 1305 1313 1318 1319 1329 1330 1335 1338 
##    0    1    0    0    1    1    1    0    1    1    1    0    1    0    0    1 
## 1339 1341 1344 1347 1349 1356 1357 1364 1367 1368 1373 1374 1375 1379 1383 1385 
##    0    1    1    0    1    0    0    0    1    1    0    0    0    1    1    0 
## 1389 1396 1404 1406 1407 1408 1410 1412 1419 1421 1423 1427 1428 1429 1432 1436 
##    1    1    0    0    0    1    0    1    0    0    0    1    0    1    0    1 
## 1443 1446 1451 1453 1457 1463 1466 1473 1475 1476 1477 1478 1479 1481 1483 1484 
##    0    0    1    1    0    0    1    0    1    1    0    1    0    1    1    0 
## 1485 1487 1492 1496 1499 1501 1507 1508 1512 1516 1520 1523 1527 1529 1533 1546 
##    0    0    1    0    1    1    1    0    0    1    1    1    1    0    1    0 
## 1547 1548 1552 1553 1558 1561 1565 1569 1571 1577 1580 1581 1599 1619 1621 1623 
##    0    1    0    0    0    1    1    1    0    0    1    1    0    0    1    0 
## 1634 1638 1640 1641 1645 1652 1665 1673 1677 1683 1689 1694 1718 1719 1723 1727 
##    1    1    0    1    0    0    1    1    1    0    0    0    1    0    0    0 
## 1729 1730 1731 1735 1736 1737 1740 1743 1746 1751 1752 1754 1760 1761 1763 1767 
##    1    0    0    1    0    1    1    0    1    1    0    0    0    1    0    1 
## 1769 1782 1783 1792 1796 1803 1806 1807 1808 1816 1821 1823 1824 1827 1830 1837 
##    0    0    0    1    0    0    1    1    1    1    0    0    0    0    0    0 
## 1839 1840 1843 1846 1849 1855 1856 1858 1864 1866 1870 1872 1876 1878 1882 1891 
##    0    0    0    0    1    0    0    0    0    0    0    1    1    0    1    0 
## 1893 1900 1904 1925 1926 1928 1929 1933 1939 1943 1956 1964 1966 1972 1975 1976 
##    1    1    1    0    1    1    1    1    1    1    1    0    0    1    0    0 
## 1981 1991 1999 2003 2004 2007 2008 2015 2017 2018 2020 2027 2028 2029 2035 2042 
##    0    1    0    1    0    0    0    0    0    0    0    0    1    0    1    0 
## 2046 2049 2050 2052 2053 2056 2073 2074 2076 2081 2083 2084 2088 2093 2098 2102 
##    1    0    0    0    0    0    0    0    1    1    1    0    0    0    0    0 
## 2107 2113 2115 2117 2120 2134 2135 2139 2140 2145 2150 2152 2154 2155 2162 2167 
##    1    0    0    0    0    0    1    0    1    1    0    0    1    1    0    1 
## 2168 2174 2175 2188 2198 2199 2203 2216 2217 2218 2222 2226 2230 2235 2241 2247 
##    0    1    1    0    0    1    0    1    0    0    0    1    1    0    1    0 
## 2253 2254 2255 2256 2259 2264 2270 2272 2275 2277 2278 2282 2283 2289 2309 2315 
##    1    0    0    1    1    0    0    1    0    0    0    0    0    0    1    0 
## 2316 2322 2327 2329 2330 2332 2338 2345 2349 2353 2356 2357 2365 2373 2376 2377 
##    1    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0 
## 2385 2389 2391 2392 2394 2400 2403 2406 2408 2412 2421 2426 2430 2431 2432 2436 
##    1    0    1    1    0    0    0    0    1    0    1    0    1    1    1    1 
## 2439 2447 2457 2460 2462 2464 2465 2466 2469 2474 2475 2479 2486 2488 2491 2504 
##    0    0    1    1    1    0    0    0    0    0    0    1    1    0    1    0 
## 2505 2510 2514 2515 2516 2520 2521 2523 2526 2531 2534 2535 2537 2538 2541 2542 
##    0    0    0    1    0    0    0    0    0    0    1    1    0    0    0    1 
## 2546 2554 2563 2574 2579 2580 2581 2582 2586 2591 2594 2595 2598 2601 2610 2615 
##    0    0    0    1    1    0    0    0    0    1    1    1    1    0    0    0 
## 2619 2626 2627 2631 2632 2639 2640 2644 2645 2647 2649 2656 2657 2658 2659 2661 
##    1    0    1    0    1    0    0    0    1    0    1    1    1    0    0    1 
## 2662 2669 2672 2675 2679 2688 2693 2695 2696 2699 2703 2704 2712 2722 2736 2744 
##    1    0    0    1    0    0    0    1    0    1    1    1    1    0    0    1 
## 2746 2747 2751 2753 2754 2755 2758 2763 2767 2768 2769 2770 2771 2780 2784 2785 
##    1    1    0    0    0    1    0    0    0    1    0    0    0    0    0    1 
## 2787 2793 2798 2803 2804 2807 2810 2812 2820 2825 2832 2834 2839 2843 2848 2850 
##    0    0    1    1    1    0    1    0    0    0    0    1    0    1    0    1 
## 2856 2866 2872 2873 2878 2879 2883 2892 2899 2900 2902 2905 2906 2915 2916 2923 
##    0    0    0    0    1    1    1    0    1    0    0    1    0    1    1    1 
## 2927 2934 2936 2940 2943 2945 2949 2951 2957 2959 2962 2963 2965 2967 2981 2984 
##    1    1    0    1    1    1    0    1    0    0    1    0    0    1    1    0 
## 2985 2989 3004 3011 3017 3028 3037 3042 3044 3050 3058 3059 3061 3064 3067 3068 
##    0    1    1    0    1    1    1    0    1    0    0    0    0    0    0    1 
## 3071 3074 3075 3076 3082 3089 3093 3095 3098 3101 3102 3105 3106 3111 3119 3127 
##    1    0    0    0    1    0    0    0    0    0    1    0    1    0    1    0 
## 3134 3136 3137 3147 3151 3160 3172 3173 3180 3181 3185 3186 3189 3193 3196 3198 
##    0    0    1    0    1    0    1    0    0    1    0    0    0    0    0    1 
## 3204 3206 3210 3212 3217 3227 3231 3233 3238 3240 3243 3245 3246 3249 3254 3257 
##    1    0    0    1    1    0    0    0    1    1    0    0    0    1    0    0 
## 3259 3265 3272 3282 3285 3289 3294 3305 3310 3323 3325 3328 3329 3330 3339 3341 
##    1    0    1    0    1    0    1    0    0    0    0    0    0    1    1    0 
## 3344 3348 3350 3354 3356 3357 3359 3363 3364 3374 3384 3387 3395 3399 3402 3405 
##    0    0    1    0    0    1    0    0    1    1    0    0    0    0    0    1 
## 3408 3409 3416 3417 3419 3427 3430 3431 3432 3434 3435 3438 3441 3450 3461 3462 
##    0    0    0    1    0    1    0    0    0    0    0    1    1    0    1    1 
## 3464 3466 3467 3477 3478 3483 3490 3493 3499 3503 3506 3510 3514 3523 3524 3533 
##    1    1    1    1    0    1    0    0    0    1    1    0    0    0    0    1 
## 3534 3541 3542 3544 3545 3548 3549 3551 3558 3559 3563 3579 3580 3583 3585 3587 
##    0    1    0    0    1    0    1    1    1    0    0    1    1    1    0    1 
## 3590 3591 3592 3596 3597 3598 3602 3603 3608 3610 3613 3620 3621 3624 3626 3630 
##    1    0    1    1    0    0    1    1    1    1    1    0    0    0    0    1 
## 3631 3641 3646 3647 3650 3654 3659 3660 3664 3676 3689 3692 3695 3700 3706 3707 
##    1    0    0    0    0    1    1    1    0    0    1    0    1    1    0    1 
## 3711 3714 3715 3724 3726 3727 3741 3742 3749 3755 3756 3758 3761 3763 3766 3775 
##    1    0    0    0    1    1    1    0    0    1    0    0    0    1    1    0 
## 3778 3779 3781 3782 3789 3795 3801 3808 3819 3823 3824 3833 3836 3839 3841 3846 
##    1    1    1    1    0    1    0    0    0    0    0    0    0    1    0    1 
## 3847 3851 3872 3877 3878 3881 3882 3883 3893 3895 3896 3898 3899 3900 3906 3914 
##    0    0    1    1    1    0    1    0    1    0    1    0    0    0    0    0 
## 3917 3924 3927 3928 3937 3938 3941 3949 3951 3952 3958 3960 3962 3963 3965 3967 
##    0    1    0    1    1    0    0    0    1    1    1    0    0    1    1    0 
## 3969 3970 3972 3973 3974 3981 3982 3993 3996 3997 4002 4004 4006 4007 4015 4017 
##    0    1    1    1    0    0    0    0    0    0    1    0    1    0    1    0 
## 4018 4021 4033 4034 4047 4048 4056 4060 4065 4066 4069 4075 4076 4077 4078 4083 
##    1    0    0    0    0    0    1    0    0    0    0    0    0    0    0    0 
## 4088 4090 4092 4095 4097 4100 4108 4109 4110 4112 4113 4122 4127 4131 4134 4135 
##    0    1    1    0    0    0    1    0    1    1    1    0    1    1    0    0 
## 4136 4138 4139 4140 4150 4151 4152 4157 4158 4162 4166 4167 4169 4171 4175 4178 
##    1    0    0    0    0    1    0    1    0    0    0    0    1    1    1    1 
## 4184 4185 4190 4191 4197 4204 4205 4208 4209 4210 4212 4215 4217 4218 4219 4225 
##    0    0    0    1    0    0    0    1    1    0    0    0    0    1    1    1 
## 4236 4238 4241 4243 4245 4247 4248 4249 4256 4259 4266 4269 4271 4280 4282 4283 
##    1    1    1    0    1    1    0    0    0    1    1    0    1    1    1    0 
## 4287 4290 4291 4293 4317 4326 4329 4333 4334 4337 4340 4345 4353 4359 4364 4366 
##    0    1    0    1    0    0    0    1    0    0    0    1    1    0    0    1 
## 4371 4374 4377 4378 4379 4381 4388 4390 4391 4396 4403 4407 4409 4413 4415 4417 
##    0    1    0    0    1    0    1    0    1    0    0    0    1    0    0    0 
## 4419 4423 4424 4427 4428 4431 4433 4438 4439 4440 4444 4445 4448 4456 4461 4464 
##    1    1    1    0    1    0    0    0    0    0    0    1    0    0    1    0 
## 4469 4479 4481 4482 4484 4485 4486 4489 4492 4498 4502 4505 4507 4509 4510 4512 
##    0    0    0    1    1    0    0    0    0    0    1    0    0    0    1    0 
## 4517 4520 4523 4524 4526 4533 4534 4540 4542 4548 4552 4560 4572 4578 4582 4585 
##    1    0    1    0    0    0    1    0    1    1    0    1    1    0    1    1 
## 4596 4625 4630 4633 4642 4654 4656 4670 4674 4678 4688 4693 4698 4703 4715 4716 
##    1    0    1    1    1    1    1    0    1    1    1    0    1    1    0    1 
## 4722 4723 4731 4739 4742 4746 4748 4753 4757 4759 4763 4771 4775 4778 4780 4783 
##    0    0    0    0    1    0    1    1    0    1    0    1    0    0    0    0 
## 4784 4786 4787 4788 4801 4806 4807 4811 4817 4826 4831 4840 4845 4854 4858 4863 
##    1    0    0    0    1    1    0    1    0    0    0    1    1    0    1    1 
## 4868 4870 4871 4877 4879 4889 4890 4891 4899 4902 4905 4906 4908 4913 4917 4919 
##    0    0    0    1    0    0    0    0    1    0    1    0    1    0    0    0 
## 4924 4925 4926 4927 4935 4936 4937 4938 4939 4940 4946 4947 4950 4959 4965 4966 
##    1    0    1    0    0    0    1    1    0    1    0    0    0    0    0    1 
## 4968 4970 4972 4978 4980 4984 4987 4988 4989 4991 5000 5003 5006 5009 5011 5014 
##    0    1    0    0    0    1    0    0    1    0    0    0    0    0    1    0 
## 5016 5017 5020 5021 5029 5039 5041 5051 5056 5059 5064 5070 5074 5075 5087 5089 
##    1    0    1    0    1    0    0    0    1    1    0    1    1    1    0    0 
## 5093 5095 5100 5101 5116 5123 5124 5127 5131 5133 5134 5137 5138 5140 5144 5153 
##    0    0    1    1    1    1    1    0    1    1    0    1    1    1    0    1 
## 5154 5163 5166 5172 5186 5188 5189 5193 5194 5197 5204 5206 5211 5214 5215 5216 
##    1    0    0    0    1    0    0    0    0    0    0    1    1    0    0    0 
## 5218 5226 5227 5230 5236 5244 5245 5248 5250 5251 5254 5261 5262 5264 5265 5268 
##    1    0    1    1    0    1    0    0    0    1    0    0    1    1    1    0 
## 5269 5273 5275 5276 5280 5283 5284 5294 5295 5296 5300 5303 5305 5310 5330 5335 
##    0    0    0    0    0    1    1    1    1    1    0    0    1    1    0    0 
## 5352 5353 5355 5357 5358 5359 5360 5363 5365 5366 5369 5371 5376 5378 5382 5385 
##    0    0    1    0    1    1    0    0    1    0    0    0    0    1    0    1 
## 5389 5393 5400 5402 5405 5406 5407 5410 5411 5417 5418 5419 5431 5432 5433 5442 
##    1    1    1    0    1    0    1    0    0    0    1    1    0    1    1    1 
## 5449 5452 5454 5455 5462 5466 5472 5473 5474 5483 5491 5495 5503 5508 5510 5511 
##    0    1    1    0    1    0    1    1    1    1    0    1    0    0    1    1 
## 5513 5515 5520 5524 5529 5539 5545 5550 5551 5554 5566 5584 5589 5599 5602 5604 
##    0    0    1    1    1    0    0    0    0    1    0    0    0    0    1    0 
## 5607 5608 5614 5616 5621 5622 5628 5638 5642 5651 5654 5662 5666 5675 5676 5691 
##    1    1    0    0    1    1    0    0    1    0    0    0    0    0    1    0 
## 5693 5704 5707 5709 5710 5715 5718 5720 5723 5726 5729 5732 5736 5739 5740 5745 
##    1    0    0    1    0    0    0    1    0    1    0    1    0    0    0    1 
## 5746 5751 5752 5757 5758 5759 5765 5770 5772 5777 5779 5780 5782 5797 5806 5810 
##    1    0    1    1    1    0    0    0    1    0    1    0    0    1    0    0 
## 5811 5814 5820 5828 5835 5849 5857 5862 5865 5867 5868 5871 5875 5878 5881 5882 
##    1    0    0    0    0    1    1    0    0    1    1    1    0    0    0    0 
## 5887 5893 5901 5904 5909 5911 5922 5923 5924 5925 5930 5931 5932 5934 5937 5941 
##    0    1    0    0    0    0    0    0    1    0    0    1    1    0    1    0 
## 5946 5949 5951 5953 5963 5973 5977 5978 5980 5984 5993 5995 5998 6000 6004 6008 
##    1    0    1    0    0    0    0    1    1    1    0    0    1    0    0    0 
## 6015 6016 6018 6030 6035 6044 6047 6049 6050 6052 6056 6057 6059 6063 6065 6066 
##    1    1    0    0    0    1    1    0    1    1    0    0    1    0    0    0 
## 6068 6069 6076 6096 6097 6099 6100 6103 6106 6109 6115 6116 6117 6119 6121 6125 
##    0    0    1    1    0    1    1    1    1    1    0    0    0    0    0    1 
## 6129 6142 6143 6146 6148 6158 6159 6167 6168 6169 6177 6185 6190 6193 6196 6199 
##    1    0    0    1    0    0    1    1    0    1    1    1    0    0    0    1 
## 6200 6206 6211 6219 6221 6222 6223 6224 6228 6235 6237 6242 6246 6250 6258 6264 
##    1    1    0    1    1    0    1    1    0    0    1    0    0    0    0    0 
## 6265 6268 6272 6275 6284 6285 6290 6293 6294 6296 6306 6317 6324 6325 6328 6334 
##    0    0    1    0    0    1    0    0    1    0    1    0    1    1    0    1 
## 6337 6340 6344 6348 6350 6352 6353 6354 6358 6360 6361 6362 6363 6379 6380 6391 
##    1    1    0    0    1    0    0    1    0    1    0    1    1    0    0    0 
## 6393 6397 6400 6411 6415 6418 6419 6422 6423 6430 6437 6440 6444 6451 6460 6462 
##    0    1    1    1    0    0    0    1    1    0    0    0    0    1    0    0 
## 6463 6465 6466 6467 6469 6471 6477 6478 6481 6483 6485 6486 6487 6488 6489 6490 
##    0    1    0    1    1    0    1    0    1    0    0    0    1    0    0    0 
## 6491 6492 6497 6499 6502 6508 6515 6521 6522 6527 6532 6534 6535 6538 6549 6551 
##    0    0    0    0    1    0    0    1    0    1    1    1    1    1    1    0 
## 6553 6555 6559 6561 6563 6565 6566 6569 6575 6580 6582 6586 6614 6615 6617 6618 
##    1    1    0    1    0    1    0    1    0    1    1    0    1    1    0    1 
## 6620 6632 6646 6654 6658 6661 6663 6664 6665 6673 6677 6681 6682 6686 6687 6690 
##    0    0    0    0    0    0    0    1    1    0    0    1    1    0    0    1 
## 6693 6694 6695 6697 6698 6702 6705 6717 6724 6727 6728 6730 6732 6733 6738 6740 
##    1    1    1    1    0    0    1    1    1    0    1    1    1    1    1    1 
## 6743 6751 6756 6757 6760 6763 6767 6770 6771 6777 6782 6783 6784 6788 6790 6793 
##    1    0    0    0    1    1    0    0    0    1    1    1    0    0    0    0 
## 6797 6798 6803 6807 6813 6815 6825 6832 6837 6848 6850 6852 6855 6859 6864 6868 
##    0    0    1    0    1    0    0    1    1    0    0    0    0    0    1    0 
## 6871 6873 6885 6887 6891 6895 6900 6903 6909 6920 6922 6926 6930 6932 6941 6952 
##    1    1    1    0    0    1    0    0    1    0    0    0    0    1    0    0 
## 6953 6958 6961 6964 6974 6976 6977 6978 6979 6981 6986 6993 7002 7005 7015 7020 
##    0    1    0    0    0    0    0    0    1    1    0    1    0    1    1    1 
## 7022 7026 7028 7039 7043 7047 7058 7073 7074 7081 7083 7084 7085 7086 7087 7089 
##    0    0    0    0    0    0    1    0    1    0    0    0    0    1    0    1 
## 7094 7097 7098 7104 7110 7115 7118 7124 7129 7133 7138 7146 7155 7160 7164 7165 
##    1    0    0    0    1    1    1    0    1    1    1    0    0    0    1    0 
## 7169 7172 7173 7180 7187 7194 7196 7200 7202 7203 7204 7211 7215 7217 7220 7226 
##    1    0    0    0    0    0    1    0    1    0    0    0    1    0    0    0 
## 7237 7240 7241 7244 7248 7249 7254 7263 7264 7265 7267 7268 7271 7273 7274 7277 
##    1    1    1    0    1    1    1    1    1    0    1    0    1    0    0    0 
## 7278 7283 7289 7295 7298 7300 7311 7322 7325 7326 7327 7332 7337 7343 7344 7345 
##    1    0    0    0    1    0    0    1    1    0    0    1    1    1    0    0 
## 7347 7348 7351 7355 7357 7363 7366 7367 7368 7372 7374 7380 7383 7389 7391 7393 
##    0    0    0    1    1    0    1    0    0    0    1    0    1    0    0    0 
## 7396 7402 7405 7409 7415 7418 7425 7428 7433 7435 7438 7439 7445 7448 7457 7458 
##    1    0    1    0    1    0    0    0    1    0    0    0    1    1    0    0 
## 7463 7465 7470 7471 7475 7476 7483 7484 7487 7489 7491 7493 7494 7495 7498 7504 
##    0    1    0    1    0    0    1    0    1    0    0    1    0    0    0    1 
## 7508 7509 7521 7523 7524 7527 7538 7542 7546 7547 7553 7560 7570 7571 7578 7579 
##    0    1    0    1    0    1    0    1    1    1    0    0    0    0    0    1 
## 7583 7584 7593 7597 7603 7606 7608 7609 7615 7618 7620 7628 7630 7631 7637 7638 
##    0    0    0    1    1    0    1    0    0    0    0    1    0    0    0    0 
## 7640 7642 7644 7649 7653 7663 7668 7669 7671 7672 7673 7677 7682 7684 7686 7687 
##    0    1    1    1    0    0    0    1    0    0    1    0    1    0    0    1 
## 7691 7694 7698 7701 7702 7718 7721 7722 7724 7725 7738 7742 7744 7745 7747 7756 
##    1    0    0    0    0    0    0    0    1    0    1    1    1    1    1    1 
## 7760 7767 7768 7771 7772 7774 7783 7785 7787 7788 7793 7798 7799 7801 7805 7808 
##    0    1    1    1    1    1    0    0    1    1    0    0    0    1    1    1 
## 7813 7815 7821 7824 7829 7832 7839 7840 7845 7847 7850 7851 7865 7868 7874 7879 
##    0    1    0    0    0    1    1    1    1    0    1    0    1    0    1    0 
## 7881 7882 7884 7886 7889 7890 7893 7902 7904 7908 7912 7918 7920 7921 7925 7930 
##    1    0    1    0    1    0    0    1    1    0    1    0    0    0    1    0 
## 7931 7937 7938 7947 7948 7952 7960 7962 7970 7972 7973 7975 7978 7979 7980 7983 
##    0    0    0    1    1    0    1    1    0    0    1    0    0    1    1    1 
## 7985 7987 7988 7991 8000 8002 8015 8019 8021 8025 8033 8036 8045 8051 8052 8056 
##    0    1    1    1    0    1    1    0    0    1    1    0    1    1    0    0 
## 8057 8059 8063 8067 8078 8079 8080 8083 8087 8091 8096 8099 8104 8108 8109 8112 
##    1    0    1    0    1    0    0    0    0    0    0    0    0    0    1    0 
## 8124 8136 8138 8140 8142 8144 8147 8149 8153 8154 8157 8160 8170 8176 8183 8206 
##    0    0    0    1    0    0    0    0    1    1    1    1    0    1    1    0 
## 8207 8209 8211 8216 8230 8241 8245 8249 8255 8265 8266 8267 8273 8284 8285 8289 
##    0    1    0    0    0    0    0    0    0    0    0    1    1    1    1    0 
## 8294 8297 8303 8305 8319 8320 8322 8325 8330 8335 8339 8340 8341 8349 8357 8358 
##    0    0    0    1    1    1    1    0    1    1    0    1    1    0    0    0 
## 8366 8371 8372 8373 8374 8386 8389 8396 8399 8412 8413 8425 8429 8434 8438 8440 
##    0    0    1    0    1    0    0    0    0    0    0    0    1    1    1    1 
## 8446 8451 8453 8454 8462 8464 8469 8472 8473 8479 8481 8487 8491 8494 8499 8501 
##    0    1    0    1    0    0    0    0    1    0    1    0    0    1    1    0 
## 8504 8514 8521 8526 
##    1    0    1    1 
## Levels: 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Making the Confusion Matrix}
\NormalTok{cm =}\StringTok{ }\KeywordTok{table}\NormalTok{(test_set[, }\DecValTok{17}\NormalTok{], y_pred)}
\NormalTok{cm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    y_pred
##        0    1
##   0 1092  148
##   1  144  748
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\StringTok{"=====================================Random Forest====================================="}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "=====================================Random Forest====================================="
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(lattice)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{confusionMatrix}\NormalTok{(cm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##    y_pred
##        0    1
##   0 1092  148
##   1  144  748
##                                           
##                Accuracy : 0.863           
##                  95% CI : (0.8477, 0.8774)
##     No Information Rate : 0.5797          
##     P-Value [Acc > NIR] : <2e-16          
##                                           
##                   Kappa : 0.7188          
##                                           
##  Mcnemar's Test P-Value : 0.8606          
##                                           
##             Sensitivity : 0.8835          
##             Specificity : 0.8348          
##          Pos Pred Value : 0.8806          
##          Neg Pred Value : 0.8386          
##              Prevalence : 0.5797          
##          Detection Rate : 0.5122          
##    Detection Prevalence : 0.5816          
##       Balanced Accuracy : 0.8592          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#=================================================================================================================}
\CommentTok{# Logistic Regression}
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}
\CommentTok{# Importing the dataset}
\NormalTok{dataset =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'train.csv'}\NormalTok{)}

\NormalTok{dataset}\OperatorTok{$}\NormalTok{days_elapsed_old[dataset}\OperatorTok{$}\NormalTok{days_elapsed_old}\OperatorTok{<}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
\NormalTok{dataset[ dataset }\OperatorTok{==}\StringTok{ "na"}\NormalTok{ ] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{#Factor like columns}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{job=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{job))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{marital=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{marital))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{education=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{education))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{device=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{device))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{outcome_old=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{outcome_old))}
\NormalTok{dataset[}\KeywordTok{is.na}\NormalTok{(dataset)] <-}\StringTok{ }\DecValTok{0}

\CommentTok{# Encoding the target feature as factor}

\CommentTok{# Splitting the dataset into the Training set and Test set}
\CommentTok{# install.packages('caTools')}
\KeywordTok{library}\NormalTok{(caTools)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{SplitRatio =} \FloatTok{0.75}\NormalTok{)}
\NormalTok{training_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE}\NormalTok{)}
\NormalTok{test_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Feature Scaling}
\NormalTok{training_set[,}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(training_set[,}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{])}
\NormalTok{test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{]) }\CommentTok{#removes third column alone}

\CommentTok{#fitting logistic regression to the training set}
\NormalTok{classifier =}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ y }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                 \DataTypeTok{family =}\NormalTok{ binomial, }\CommentTok{#for logistic reg mention binomial}
                 \DataTypeTok{data =}\NormalTok{ training_set)}

\CommentTok{#predicting the test set results}
\NormalTok{prob_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{type =} \StringTok{'response'}\NormalTok{,}\DataTypeTok{newdata =}\NormalTok{ test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}\CommentTok{#use type = response for logistic reg}
\NormalTok{prob_pred                                                          }\CommentTok{#that will give the prob listed in the single vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            2            5            6            7           11           15 
## 0.8427212359 0.8821662255 0.9028108432 0.0186598106 0.2169430278 0.3187913640 
##           16           18           20           21           31           36 
## 0.4782303985 0.0252262726 0.8918778421 0.0823708773 0.5398358529 0.9370782562 
##           37           41           42           43           56           59 
## 0.8913892844 0.4664663820 0.1313229621 0.9997663729 0.5743939009 0.3115089978 
##           63           64           67           69           83           86 
## 0.7937269908 0.0859625570 0.4087884255 0.1425132971 0.0422665782 0.4473865664 
##           89          101          110          115          124          125 
## 0.2757521457 0.9812313577 0.1740192571 0.0947024431 0.7163243271 0.4165784021 
##          128          140          143          148          149          151 
## 0.4296399412 0.0335868187 0.0319389717 0.1773857982 0.5745044887 0.3250740408 
##          154          155          158          160          163          164 
## 0.7786082651 0.3085817106 0.9695318767 0.0662565336 0.5687672687 0.4890073552 
##          165          167          169          177          179          182 
## 0.2853670543 0.4348449232 0.6442354423 0.0745776835 0.6486269487 0.1857915133 
##          183          187          194          197          201          202 
## 0.0708530589 0.1128402316 0.0491328778 0.0802314221 0.1471116990 0.3279233390 
##          205          211          217          218          220          225 
## 0.0334192875 0.8710546893 0.6143187140 0.9413090392 0.0947044261 0.3578358306 
##          226          229          236          245          247          252 
## 0.0641709178 0.0363876683 0.0775469253 0.2908783168 0.0921471272 0.1170099652 
##          254          256          261          264          267          270 
## 0.4497986798 0.2034595580 0.4151035300 0.0190968460 0.0240391340 0.9215563484 
##          274          279          283          290          291          293 
## 0.0759669296 0.9394466496 0.0393926463 0.0361543395 0.3344552201 0.0648951230 
##          298          301          303          314          321          322 
## 0.1261714249 0.1087129081 0.9863396872 0.9277931228 0.8939585959 0.4903711819 
##          323          324          326          334          339          341 
## 0.4928253065 0.7112224833 0.2807707676 0.4819204009 0.4355484823 0.1071816304 
##          346          349          350          351          353          365 
## 0.2636266654 0.1807861423 0.0884365856 0.1168121487 0.0544154271 0.0674707529 
##          366          367          371          374          383          385 
## 0.2522105678 0.0269585568 0.2242389735 0.1719779149 0.4713247761 0.3476375054 
##          389          397          399          404          405          406 
## 0.7266221212 0.0747423394 0.9435962397 0.2522363460 0.5452210213 0.9728591354 
##          407          413          414          420          421          422 
## 0.0288847232 0.0305090071 0.7126591138 0.5318296010 0.9504087249 0.1978523459 
##          438          439          445          446          449          450 
## 0.9997068637 0.7108999840 0.0257316855 0.0250372955 0.9047651061 0.0199325408 
##          456          464          468          474          476          481 
## 0.8836093246 0.5124794466 0.9560729935 0.9956181875 0.1860956286 0.5639502105 
##          485          499          500          506          509          512 
## 0.5851835588 0.1785487766 0.6310813306 0.9229876968 0.9985369873 0.1843733740 
##          513          520          524          528          531          532 
## 0.6237993704 0.3901467037 0.0561138441 0.0658393600 0.9938732259 0.2421568121 
##          535          537          540          551          552          554 
## 0.1477377347 0.0487187701 0.7761498993 0.9558383406 0.1288449874 0.6868315254 
##          555          559          560          570          573          574 
## 0.1430898977 0.3054583838 0.2362215718 0.0355941389 0.9909474463 0.9560547912 
##          587          589          598          599          603          604 
## 0.1817102042 0.1554311241 0.7838649810 0.6184195435 0.4718718522 0.1898879173 
##          605          607          608          616          622          626 
## 0.1146138127 0.9925906279 0.5300779790 0.9907058228 0.7309126164 0.5722619669 
##          633          634          637          638          649          654 
## 0.1408050483 0.3626958523 0.5883584610 0.3926272572 0.1951526293 0.1938996218 
##          655          656          657          662          666          667 
## 0.0921989075 0.0292796161 0.3893592674 0.1298407846 0.7935512233 0.4043473978 
##          668          669          673          674          675          676 
## 0.6969627490 0.9889617743 0.1029577406 0.4891231419 0.2064024618 0.8422276778 
##          682          683          685          689          692          705 
## 0.8304146710 0.1806566008 0.9564386029 0.1178877679 0.4013439847 0.0607249397 
##          711          717          719          721          725          726 
## 0.3550598446 0.0464215041 0.6235766711 0.9232803431 0.2141630071 0.7083231863 
##          727          729          731          734          737          741 
## 0.2869430641 0.7155059260 0.3685152170 0.1806942522 0.1717116056 0.2229360731 
##          745          751          756          762          766          768 
## 0.6920740792 0.4424922894 0.5680989909 0.2619537970 0.0228702374 0.2458887409 
##          774          775          778          780          784          788 
## 0.5016639889 0.7452189692 0.5410156219 0.7814342533 0.1609506864 0.0215661595 
##          794          796          797          802          803          808 
## 0.4271086104 0.0222720713 0.7556824105 0.1915191897 0.0348983153 0.2146697029 
##          816          818          821          822          827          829 
## 0.9589687551 0.5050111294 0.1112132641 0.0767809496 0.0365161363 0.0524727299 
##          832          838          842          848          849          860 
## 0.8748462750 0.4699182576 0.8891908062 0.0812118239 0.0735926433 0.4656230432 
##          861          865          870          876          878          880 
## 0.0557566068 0.7051716792 0.9318625904 0.1060984608 0.1314967294 0.2905341802 
##          881          887          888          892          893          900 
## 0.2360140678 0.1477326107 0.5947489296 0.9764178255 0.1446939211 0.4160096997 
##          901          907          913          915          919          922 
## 0.8641304264 0.0617085341 0.0659973686 0.3648946791 0.8629790976 0.0910740390 
##          923          925          927          931          936          944 
## 0.1116908111 0.1253703961 0.2948226402 0.4092255974 0.7767598834 0.3765226691 
##          951          953          954          958          959          960 
## 0.4914799935 0.1091224283 0.0103772116 0.8755572843 0.3555408609 0.9324593134 
##          962          964          967          971          982          998 
## 0.0361181183 0.7103004474 0.9999892499 0.1441943894 0.1160339250 0.3229467797 
##          999         1003         1008         1013         1026         1028 
## 0.4253276635 0.7674064655 0.0088476985 0.8373842235 0.4253198204 0.1660452531 
##         1034         1042         1047         1049         1050         1056 
## 0.4097443909 0.1647724817 0.0727836466 0.5366887926 0.6808192929 0.9222773902 
##         1062         1069         1070         1081         1085         1089 
## 0.1244843092 0.0271962991 0.0960874278 0.0277840702 0.1325917066 0.3064933411 
##         1093         1096         1111         1119         1129         1131 
## 0.9392259990 0.9110331073 0.7072247691 0.0642078220 0.9523021765 0.3692241392 
##         1133         1134         1138         1140         1142         1146 
## 0.4726987178 0.3928184806 0.3995160613 0.6914438658 0.2972268619 0.2203297890 
##         1147         1148         1153         1156         1162         1167 
## 0.2485347265 0.1984151885 0.2378029833 0.2901130859 0.9780776329 0.4702735926 
##         1170         1175         1176         1182         1185         1193 
## 0.2894466912 0.2219644016 0.1234249490 0.1447607725 0.1270059839 0.4976027641 
##         1198         1202         1213         1214         1225         1232 
## 0.3797609459 0.2684518375 0.7713773280 0.6661727088 0.2170191551 0.9549262628 
##         1240         1241         1244         1249         1252         1257 
## 0.0426274297 0.4497419714 0.0914249967 0.1525578844 0.8166379190 0.8279353267 
##         1262         1266         1269         1270         1278         1284 
## 0.1604035266 0.4203999228 0.3223071987 0.6230778123 0.2486835293 0.1432506747 
##         1287         1290         1294         1303         1305         1313 
## 0.4368702189 0.9758617997 0.3779648809 0.2073209566 0.7266935901 0.6969286325 
##         1318         1319         1329         1330         1335         1338 
## 0.9916972267 0.1592206642 0.6030155395 0.1804460379 0.3319056535 0.9202467975 
##         1339         1341         1344         1347         1349         1356 
## 0.2286854102 0.8993638911 0.4250237499 0.2376842729 0.9539266428 0.1958100338 
##         1357         1364         1367         1368         1373         1374 
## 0.2896726713 0.0735401322 0.8538813145 0.6491633640 0.1184148319 0.0268417395 
##         1375         1379         1383         1385         1389         1396 
## 0.1781229313 0.6191771769 0.6563237414 0.3691258607 0.7112660397 0.5852901053 
##         1404         1406         1407         1408         1410         1412 
## 0.0312643094 0.3496367143 0.2178910196 0.5945048427 0.0454531346 0.7925047080 
##         1419         1421         1423         1427         1428         1429 
## 0.1076481738 0.7110853030 0.0639043123 0.8973427476 0.2843182283 0.9777055865 
##         1432         1436         1443         1446         1451         1453 
## 0.2633429186 0.6591995649 0.2026881767 0.1628088389 0.8940015333 0.6836270582 
##         1457         1463         1466         1473         1475         1476 
## 0.0933092172 0.0945007290 0.9994375248 0.0100189687 0.5725840576 0.9534935245 
##         1477         1478         1479         1481         1483         1484 
## 0.0662934419 0.3187350804 0.1349942921 0.7604297498 0.9998424290 0.5132665654 
##         1485         1487         1492         1496         1499         1501 
## 0.2143602871 0.0711317231 0.9980858769 0.0726554649 0.2446550204 0.9306798326 
##         1507         1508         1512         1516         1520         1523 
## 0.4695217542 0.0236461989 0.5313207212 0.9952790696 0.9944905381 0.8991978325 
##         1527         1529         1533         1546         1547         1548 
## 0.9749355245 0.0338309017 0.7134616472 0.1273410510 0.0978594805 0.6629413774 
##         1552         1553         1558         1561         1565         1569 
## 0.1321637715 0.1649268081 0.0659345068 0.9961129412 0.4656292063 0.8686744873 
##         1571         1577         1580         1581         1599         1619 
## 0.4439755099 0.3628248722 0.8225579230 0.7438543505 0.2542423470 0.0859160783 
##         1621         1623         1634         1638         1640         1641 
## 0.5863233115 0.2290719033 0.9968294764 0.3487307984 0.0534887697 0.4055182734 
##         1645         1652         1665         1673         1677         1683 
## 0.3992785358 0.0842061275 0.6315174766 0.2124532927 0.7514333976 0.1829086112 
##         1689         1694         1718         1719         1723         1727 
## 0.1443069466 0.0394893114 0.9968683082 0.2222654940 0.1891133811 0.2879375404 
##         1729         1730         1731         1735         1736         1737 
## 0.8942481348 0.3538740948 0.0443485808 0.6107358846 0.1386613420 0.5322744141 
##         1740         1743         1746         1751         1752         1754 
## 0.4130585392 0.0612076379 0.5401391171 0.7133578530 0.0513424429 0.2088307685 
##         1760         1761         1763         1767         1769         1782 
## 0.0943339904 0.9132528622 0.2293055812 0.7459048583 0.2475140695 0.2213419933 
##         1783         1792         1796         1803         1806         1807 
## 0.1630408037 0.4768674687 0.4698006257 0.1038980833 0.9400645588 0.8207205037 
##         1808         1816         1821         1823         1824         1827 
## 0.5087640920 0.5267291255 0.0724925045 0.4427365125 0.3800695806 0.0826483692 
##         1830         1837         1839         1840         1843         1846 
## 0.2480974950 0.0419572027 0.5508859607 0.3225809292 0.2323319834 0.1484657975 
##         1849         1855         1856         1858         1864         1866 
## 0.9174203904 0.2490171509 0.3840053654 0.4301348560 0.0677142983 0.2291179660 
##         1870         1872         1876         1878         1882         1891 
## 0.3681074032 0.9123854963 0.8064131735 0.0304309181 0.8115530552 0.2711860728 
##         1893         1900         1904         1925         1926         1928 
## 0.8930156631 0.8114115490 0.8817794229 0.1339574603 0.6003570547 0.6825702730 
##         1929         1933         1939         1943         1956         1964 
## 0.6451832272 0.9503656656 0.6245100653 0.5224918914 0.5576304422 0.2280296923 
##         1966         1972         1975         1976         1981         1991 
## 0.0358572495 0.5871142200 0.4211996623 0.0501034202 0.2754341083 0.4527744566 
##         1999         2003         2004         2007         2008         2015 
## 0.0401270557 0.4685530412 0.0194887838 0.5071606176 0.0222849321 0.0727350251 
##         2017         2018         2020         2027         2028         2029 
## 0.1926296044 0.0863218798 0.6082979470 0.2434299947 0.9627867061 0.1136993008 
##         2035         2042         2046         2049         2050         2052 
## 0.8086635572 0.0154586337 0.6276217442 0.1412160369 0.0979448521 0.0272088834 
##         2053         2056         2073         2074         2076         2081 
## 0.1479069796 0.2650065141 0.0822077558 0.0709180660 0.9100098313 0.4298276841 
##         2083         2084         2088         2093         2098         2102 
## 0.9019560533 0.0242904176 0.2427731595 0.1396748569 0.0289359568 0.1520883438 
##         2107         2113         2115         2117         2120         2134 
## 0.8285441382 0.5602458251 0.0422436341 0.2995303126 0.0934076216 0.0776665766 
##         2135         2139         2140         2145         2150         2152 
## 0.8966785017 0.2914481999 0.9851601230 0.8528181831 0.2266910292 0.0272487060 
##         2154         2155         2162         2167         2168         2174 
## 0.4489733428 0.7601137160 0.0687093613 0.7508212237 0.0506460273 0.8957225793 
##         2175         2188         2198         2199         2203         2216 
## 0.6952050234 0.1806210718 0.2485281308 0.3381455444 0.2662131669 0.4777250853 
##         2217         2218         2222         2226         2230         2235 
## 0.5395849862 0.2546567994 0.1286297806 0.4938980711 0.7966397403 0.0641362168 
##         2241         2247         2253         2254         2255         2256 
## 0.6081628295 0.2743655682 0.7125205463 0.8052577672 0.0950199640 0.3102040875 
##         2259         2264         2270         2272         2275         2277 
## 0.7561643976 0.6038453448 0.2608721981 0.8243596385 0.1876170201 0.0502255464 
##         2278         2282         2283         2289         2309         2315 
## 0.2927905966 0.0584992730 0.3293507268 0.1067857007 0.8675376222 0.0615541029 
##         2316         2322         2327         2329         2330         2332 
## 0.9053783584 0.5192514381 0.0775227491 0.3392253329 0.3115180999 0.0957362523 
##         2338         2345         2349         2353         2356         2357 
## 0.3516060132 0.9879875884 0.0273683210 0.0822066435 0.8389277296 0.3099140395 
##         2365         2373         2376         2377         2385         2389 
## 0.2224820690 0.0028610951 0.3369252831 0.0917237933 0.9819106388 0.2099796521 
##         2391         2392         2394         2400         2403         2406 
## 0.8439687008 0.5553824519 0.0937385325 0.0708382102 0.0940618298 0.2408779424 
##         2408         2412         2421         2426         2430         2431 
## 0.6911267050 0.0988565615 0.5400341764 0.0518749851 0.4292523328 0.4754673027 
##         2432         2436         2439         2447         2457         2460 
## 0.8413828114 0.8508754431 0.3200916623 0.0516918664 0.9194538571 0.7791254208 
##         2462         2464         2465         2466         2469         2474 
## 0.2123976274 0.2525073137 0.1513729360 0.1854977668 0.1859640130 0.0134912665 
##         2475         2479         2486         2488         2491         2504 
## 0.4181681446 0.9577123014 0.9959496987 0.1217473607 0.4522092756 0.1234472670 
##         2505         2510         2514         2515         2516         2520 
## 0.1769548853 0.7550666412 0.2230115425 0.9784893996 0.1556974514 0.2169290156 
##         2521         2523         2526         2531         2534         2535 
## 0.5860864120 0.0731200742 0.0236594116 0.1500207504 0.8727439548 0.9502002077 
##         2537         2538         2541         2542         2546         2554 
## 0.0735093660 0.5753222426 0.0537980214 0.3285734914 0.1984301975 0.1470841758 
##         2563         2574         2579         2580         2581         2582 
## 0.7318241949 0.9486717821 0.9504032611 0.3392017115 0.0244135958 0.2295738655 
##         2586         2591         2594         2595         2598         2601 
## 0.4236747566 0.3827888793 0.9212544573 0.9881492377 0.4269532214 0.2590609454 
##         2610         2615         2619         2626         2627         2631 
## 0.0716685989 0.2525619143 0.3992998690 0.0085681920 0.0312573333 0.1107583299 
##         2632         2639         2640         2644         2645         2647 
## 0.8562248544 0.0228084708 0.0871187328 0.2728003155 0.9393093219 0.0582564607 
##         2649         2656         2657         2658         2659         2661 
## 0.6856478355 0.2969946438 0.9886735129 0.1262449353 0.3296186132 0.0746366892 
##         2662         2669         2672         2675         2679         2688 
## 0.4905002419 0.1852335816 0.0662754215 0.9096799402 0.2067487864 0.1897091436 
##         2693         2695         2696         2699         2703         2704 
## 0.1085576624 0.2528134131 0.4093569565 0.8618764992 0.8409078349 0.5463065118 
##         2712         2722         2736         2744         2746         2747 
## 0.4197600909 0.3744528275 0.2435908490 0.6592382662 0.8906925080 0.9837748701 
##         2751         2753         2754         2755         2758         2763 
## 0.0328041521 0.3357268798 0.1575353229 0.9993650028 0.2963876886 0.0740177496 
##         2767         2768         2769         2770         2771         2780 
## 0.2076164351 0.8406993826 0.0554582986 0.0919165658 0.0425595064 0.6657924411 
##         2784         2785         2787         2793         2798         2803 
## 0.1187566629 0.5884104387 0.0530951677 0.0948859038 0.8953870671 0.5505200104 
##         2804         2807         2810         2812         2820         2825 
## 0.6785942643 0.0401199205 0.3798595000 0.1182444526 0.0569809456 0.0232915532 
##         2832         2834         2839         2843         2848         2850 
## 0.0685381114 0.4882129302 0.1310920672 0.4897079098 0.0664784593 0.7266160145 
##         2856         2866         2872         2873         2878         2879 
## 0.0807471388 0.0399530629 0.1603880617 0.2348782158 0.8555957539 0.8307386546 
##         2883         2892         2899         2900         2902         2905 
## 0.4959174261 0.4789854385 0.8348903245 0.2516198999 0.2562131258 0.8934900633 
##         2906         2915         2916         2923         2927         2934 
## 0.2143640274 0.8686909723 0.5354828604 0.4183308875 0.9271098187 0.7310658442 
##         2936         2940         2943         2945         2949         2951 
## 0.0138530254 0.7183457748 0.9587591670 0.3883120194 0.0514534096 0.5728270599 
##         2957         2959         2962         2963         2965         2967 
## 0.3302364489 0.3071722666 0.9197506505 0.3551373043 0.1733677362 0.3346978440 
##         2981         2984         2985         2989         3004         3011 
## 0.5893412667 0.1394203416 0.1753264450 0.4744124471 0.6311608735 0.0908300876 
##         3017         3028         3037         3042         3044         3050 
## 0.7623452343 0.9313438968 0.8457610970 0.0535285187 0.8335025441 0.2272082406 
##         3058         3059         3061         3064         3067         3068 
## 0.1981258950 0.0557004971 0.2788578199 0.0774541850 0.1431671221 0.5990807697 
##         3071         3074         3075         3076         3082         3089 
## 0.5416465378 0.2494667539 0.1160683618 0.2392721122 0.9602626701 0.0612276868 
##         3093         3095         3098         3101         3102         3105 
## 0.1727674829 0.0219640226 0.3351571968 0.3213291192 0.8679527541 0.3401627658 
##         3106         3111         3119         3127         3134         3136 
## 0.5639532595 0.2240443390 0.9926235035 0.2897832696 0.4689467574 0.2537888393 
##         3137         3147         3151         3160         3172         3173 
## 0.9426323927 0.3731398426 0.6994971159 0.3333262218 0.8482817460 0.0413185870 
##         3180         3181         3185         3186         3189         3193 
## 0.0623718474 0.7769473994 0.3361783893 0.1077812142 0.0461094719 0.0235834118 
##         3196         3198         3204         3206         3210         3212 
## 0.0961695384 0.5577562773 0.9879483370 0.2106631918 0.0119392229 0.4049746425 
##         3217         3227         3231         3233         3238         3240 
## 0.3014503211 0.0945538469 0.1788508616 0.2909468692 0.7767541440 0.9079793509 
##         3243         3245         3246         3249         3254         3257 
## 0.0636548451 0.0481725984 0.1020985465 0.2060419259 0.0781750946 0.1506296300 
##         3259         3265         3272         3282         3285         3289 
## 0.9941212502 0.1523289468 0.3668281833 0.2153783366 0.5523311242 0.3457874938 
##         3294         3305         3310         3323         3325         3328 
## 0.9250561286 0.3191675595 0.1995700605 0.1847409032 0.0415710905 0.1275260573 
##         3329         3330         3339         3341         3344         3348 
## 0.4353054681 0.5448268066 0.6067314503 0.0561520124 0.1288310395 0.1183128057 
##         3350         3354         3356         3357         3359         3363 
## 0.8419189361 0.1129435667 0.0937504951 0.8801542729 0.2708397972 0.0778580599 
##         3364         3374         3384         3387         3395         3399 
## 0.9114077437 0.6731301265 0.4034856566 0.0840248946 0.0525002735 0.0912802142 
##         3402         3405         3408         3409         3416         3417 
## 0.1260280362 0.6560899420 0.3330102721 0.2710499925 0.2249028230 0.9997676671 
##         3419         3427         3430         3431         3432         3434 
## 0.2100437483 0.6260818847 0.0224688527 0.2814230395 0.2375014894 0.1436717495 
##         3435         3438         3441         3450         3461         3462 
## 0.1332316352 0.7122450506 0.8264234762 0.2848711974 0.6064780128 0.8579630297 
##         3464         3466         3467         3477         3478         3483 
## 0.9764807292 0.8546973008 0.8883855654 0.7354695581 0.4173856592 0.9755385857 
##         3490         3493         3499         3503         3506         3510 
## 0.2007675159 0.0900084534 0.0787535593 0.9699169854 0.4604403677 0.0498505265 
##         3514         3523         3524         3533         3534         3541 
## 0.0401349163 0.2484859628 0.1702881374 0.9564778650 0.1732860368 0.8152836064 
##         3542         3544         3545         3548         3549         3551 
## 0.0928077225 0.1054540146 0.8467342428 0.1822929661 0.9289419777 0.9449086954 
##         3558         3559         3563         3579         3580         3583 
## 0.9530347406 0.3996338726 0.0578139013 0.5408528684 0.8661936732 0.4714776534 
##         3585         3587         3590         3591         3592         3596 
## 0.7774036045 0.9425662432 0.7057402240 0.3227021639 0.6439194077 0.9031712273 
##         3597         3598         3602         3603         3608         3610 
## 0.1809413054 0.0156853484 0.6000962204 0.9635602512 0.9233613173 0.8112507070 
##         3613         3620         3621         3624         3626         3630 
## 0.9299077190 0.3368887782 0.0987129184 0.2046181633 0.2396969175 0.8055905685 
##         3631         3641         3646         3647         3650         3654 
## 0.0598637532 0.0359034612 0.0942105627 0.0306617740 0.1625046267 0.4641136741 
##         3659         3660         3664         3676         3689         3692 
## 0.8568937067 0.8121691560 0.1062234711 0.3971145891 0.8840727644 0.0420800474 
##         3695         3700         3706         3707         3711         3714 
## 0.8912217465 0.4270515317 0.5848266544 0.8447032929 0.8143755338 0.3317972706 
##         3715         3724         3726         3727         3741         3742 
## 0.5545959967 0.1069680011 0.9632694444 0.9601447642 0.4714199207 0.0525880494 
##         3749         3755         3756         3758         3761         3763 
## 0.0597226188 0.9724439668 0.3011939325 0.0469393822 0.2242203479 0.2878899793 
##         3766         3775         3778         3779         3781         3782 
## 0.8453655906 0.4361682628 0.7747031230 0.5352676126 0.9954185032 0.7019850442 
##         3789         3795         3801         3808         3819         3823 
## 0.0490998083 0.9056396441 0.4318038313 0.0220423738 0.0350551774 0.1150542879 
##         3824         3833         3836         3839         3841         3846 
## 0.6968603749 0.0946175663 0.0364749031 0.9646896379 0.2444582977 0.9050725699 
##         3847         3851         3872         3877         3878         3881 
## 0.2565790497 0.4168521304 0.8338718922 0.8824782089 0.9262008790 0.4444298622 
##         3882         3883         3893         3895         3896         3898 
## 0.8527917356 0.1337330351 0.4570376068 0.1486961878 0.6614409292 0.0589390356 
##         3899         3900         3906         3914         3917         3924 
## 0.1463800721 0.7531700293 0.0662393534 0.0303803573 0.1917074021 0.7375651519 
##         3927         3928         3937         3938         3941         3949 
## 0.1217837024 0.6075292391 0.9630803599 0.2392723781 0.1158625361 0.0280472580 
##         3951         3952         3958         3960         3962         3963 
## 0.8853412896 0.3544323381 0.3522116355 0.3613667539 0.0018757364 0.8375796807 
##         3965         3967         3969         3970         3972         3973 
## 0.4871592082 0.1142163430 0.3174821881 0.3494369608 0.7365097147 0.9010331184 
##         3974         3981         3982         3993         3996         3997 
## 0.2492172174 0.4857407348 0.0342498551 0.1462963401 0.2350821213 0.0149845348 
##         4002         4004         4006         4007         4015         4017 
## 0.9500916039 0.0541247325 0.9512567014 0.3563767821 0.3803485130 0.1129220120 
##         4018         4021         4033         4034         4047         4048 
## 0.8183367456 0.4350403044 0.3824755514 0.1392874069 0.1341095471 0.1405059551 
##         4056         4060         4065         4066         4069         4075 
## 0.2341726994 0.0414591257 0.2041658491 0.0267700933 0.2673546834 0.1731993548 
##         4076         4077         4078         4083         4088         4090 
## 0.1390372218 0.1012559347 0.5856361423 0.3636563509 0.1309853199 0.5702965507 
##         4092         4095         4097         4100         4108         4109 
## 0.4292284790 0.0321404067 0.1316028998 0.3155386504 0.3233328261 0.0845977365 
##         4110         4112         4113         4122         4127         4131 
## 0.8901201459 0.6497612511 0.3500260650 0.0450194133 0.9828685386 0.4670257089 
##         4134         4135         4136         4138         4139         4140 
## 0.4298620912 0.3856463463 0.8413777842 0.2794919091 0.4124368260 0.1012918358 
##         4150         4151         4152         4157         4158         4162 
## 0.3277566865 0.1213190493 0.1087050764 0.1777857165 0.1778009421 0.0783687445 
##         4166         4167         4169         4171         4175         4178 
## 0.4961566627 0.2092655522 0.9369991583 0.4357908888 0.9853278773 0.5635174516 
##         4184         4185         4190         4191         4197         4204 
## 0.0420461089 0.1235911568 0.1494940634 0.8342896677 0.0843536946 0.1289334776 
##         4205         4208         4209         4210         4212         4215 
## 0.0496792137 0.3443435574 0.4961030476 0.1336745932 0.3995040216 0.0380265385 
##         4217         4218         4219         4225         4236         4238 
## 0.1825957205 0.9376252971 0.9477572037 0.9999988732 0.9538037599 0.9691496631 
##         4241         4243         4245         4247         4248         4249 
## 0.8893724250 0.2954437341 0.9995124705 0.5102670033 0.1792658701 0.2409873504 
##         4256         4259         4266         4269         4271         4280 
## 0.0745432720 0.7911113361 0.4378311746 0.0284453578 0.8422923281 0.6969160803 
##         4282         4283         4287         4290         4291         4293 
## 0.5213929527 0.2717748863 0.0994264630 0.9987522094 0.1944170081 0.5890457113 
##         4317         4326         4329         4333         4334         4337 
## 0.3741532225 0.3015874805 0.0606849925 0.2088872423 0.0288378170 0.1851658904 
##         4340         4345         4353         4359         4364         4366 
## 0.2722593476 0.3049130176 0.6651766066 0.2196373221 0.0177696033 0.9999856334 
##         4371         4374         4377         4378         4379         4381 
## 0.0122132245 0.9694009242 0.1555391763 0.0406503762 0.9993492521 0.1015759512 
##         4388         4390         4391         4396         4403         4407 
## 0.6813052661 0.1290921899 0.4023566802 0.1733170522 0.2165043815 0.2947766029 
##         4409         4413         4415         4417         4419         4423 
## 0.9392025826 0.6053043161 0.3417191846 0.1706744621 0.9771299277 0.8440421210 
##         4424         4427         4428         4431         4433         4438 
## 0.7824580397 0.1633938603 0.3665062151 0.1560267550 0.0451435184 0.1182113415 
##         4439         4440         4444         4445         4448         4456 
## 0.0717282971 0.1953724172 0.7063857485 0.9066637858 0.4724090414 0.0713963731 
##         4461         4464         4469         4479         4481         4482 
## 0.9091448464 0.0525093524 0.3159181805 0.2860336081 0.1641413962 0.3198218788 
##         4484         4485         4486         4489         4492         4498 
## 0.9381447196 0.0292182592 0.0523313449 0.0662240143 0.3030051284 0.4086732964 
##         4502         4505         4507         4509         4510         4512 
## 0.4783155496 0.0409915903 0.0667369320 0.1416218515 0.8742573280 0.3107851229 
##         4517         4520         4523         4524         4526         4533 
## 0.9974063951 0.3703691030 0.8883882481 0.1346067632 0.1663101658 0.0915806856 
##         4534         4540         4542         4548         4552         4560 
## 0.7502477235 0.6038673221 0.7730068392 0.3199207891 0.1215470781 0.5558587330 
##         4572         4578         4582         4585         4596         4625 
## 0.5967526913 0.7936461397 0.9719128228 0.3626259653 0.7854207311 0.0226809740 
##         4630         4633         4642         4654         4656         4670 
## 0.8582837645 0.3185600082 0.8830764791 0.2901082359 0.2967615247 0.0973109569 
##         4674         4678         4688         4693         4698         4703 
## 0.6837848935 0.8432161794 0.8334425651 0.0781396834 0.9389816748 0.9978070368 
##         4715         4716         4722         4723         4731         4739 
## 0.0731227022 0.7794622587 0.1982648156 0.5841394929 0.3101475343 0.2266164383 
##         4742         4746         4748         4753         4757         4759 
## 0.9555105123 0.6248732064 0.5823512200 0.5365492341 0.1711789640 0.4544200583 
##         4763         4771         4775         4778         4780         4783 
## 0.2010082792 0.6102897037 0.4387844948 0.1654514149 0.1258391674 0.2126878797 
##         4784         4786         4787         4788         4801         4806 
## 0.9670247192 0.0800784538 0.2338408794 0.0775180306 0.6217834422 0.9309769420 
##         4807         4811         4817         4826         4831         4840 
## 0.2439174512 0.6157479418 0.4371762049 0.2608287623 0.2326006305 0.8241018340 
##         4845         4854         4858         4863         4868         4870 
## 0.6906224342 0.1588326569 0.9807735606 0.6392213550 0.0492253736 0.4611767291 
##         4871         4877         4879         4889         4890         4891 
## 0.6129428209 0.5896720145 0.1615290378 0.3915460629 0.1057443593 0.1777446848 
##         4899         4902         4905         4906         4908         4913 
## 0.9998396900 0.2778292200 0.3070768803 0.3707977483 0.3193128137 0.0815017980 
##         4917         4919         4924         4925         4926         4927 
## 0.0922103244 0.2143290319 0.7784782882 0.0939202462 0.3139402549 0.0454963259 
##         4935         4936         4937         4938         4939         4940 
## 0.0079395011 0.6370939859 0.2777620649 0.4875618415 0.1548195176 0.9761417394 
##         4946         4947         4950         4959         4965         4966 
## 0.2881037836 0.1181279051 0.2493424390 0.4046349872 0.2299220646 0.8178780062 
##         4968         4970         4972         4978         4980         4984 
## 0.2068794048 0.3317845888 0.4001730597 0.7117677264 0.2364541752 0.8774038752 
##         4987         4988         4989         4991         5000         5003 
## 0.1650018029 0.0594425474 0.6330110200 0.1593990270 0.2351575274 0.1563392808 
##         5006         5009         5011         5014         5016         5017 
## 0.1631741883 0.0380200113 0.9379977603 0.2011142950 0.8880658200 0.2370719719 
##         5020         5021         5029         5039         5041         5051 
## 0.9999490073 0.0809664365 0.9888466153 0.0648788522 0.2502088009 0.0553040465 
##         5056         5059         5064         5070         5074         5075 
## 0.7712436559 0.9525018009 0.0346193754 0.8476492540 0.8558507469 0.7916716967 
##         5087         5089         5093         5095         5100         5101 
## 0.1417330125 0.2291436262 0.2071424180 0.3775700726 0.6983452744 0.8845606935 
##         5116         5123         5124         5127         5131         5133 
## 0.8104054335 0.8180858344 0.4882710023 0.0414775075 0.7855138996 0.5293915853 
##         5134         5137         5138         5140         5144         5153 
## 0.0826812427 0.6434956578 0.5440350304 0.7886679922 0.0735338809 0.4635222148 
##         5154         5163         5166         5172         5186         5188 
## 0.8863427272 0.3770581114 0.3315216769 0.0867182807 0.9160718469 0.0392503521 
##         5189         5193         5194         5197         5204         5206 
## 0.2023397402 0.6010466752 0.2111862552 0.1160546640 0.0935222897 0.7369836053 
##         5211         5214         5215         5216         5218         5226 
## 0.9740774601 0.0505456983 0.0200509757 0.0599115749 0.9643174083 0.1017848448 
##         5227         5230         5236         5244         5245         5248 
## 0.9533737443 0.8935543970 0.3281964847 0.7662934387 0.4234872433 0.1350378206 
##         5250         5251         5254         5261         5262         5264 
## 0.2229800996 0.4917021812 0.2413669324 0.1031592507 0.6759426107 0.6234937448 
##         5265         5268         5269         5273         5275         5276 
## 0.9414893412 0.1456513789 0.3427045608 0.1152147524 0.2875529536 0.3051923815 
##         5280         5283         5284         5294         5295         5296 
## 0.6632414051 0.9653857149 0.9346929836 0.4981978529 0.4118956624 0.8510009851 
##         5300         5303         5305         5310         5330         5335 
## 0.2293930924 0.0229380202 0.6314888885 0.8843923611 0.0073008171 0.1064044041 
##         5352         5353         5355         5357         5358         5359 
## 0.0191091203 0.2772269222 0.8017274842 0.0493279297 0.5678595741 0.9913397491 
##         5360         5363         5365         5366         5369         5371 
## 0.1637793820 0.1068500649 0.7412021799 0.1719271453 0.1720576154 0.1361237639 
##         5376         5378         5382         5385         5389         5393 
## 0.1316850328 0.5673187511 0.3438157488 0.4361321401 0.9909400834 0.4316636446 
##         5400         5402         5405         5406         5407         5410 
## 0.7513054520 0.1440631952 0.9550271172 0.1996878920 0.8286866329 0.1455543626 
##         5411         5417         5418         5419         5431         5432 
## 0.2440419508 0.0692652698 0.5576201809 0.7484172947 0.0662251971 0.5275204794 
##         5433         5442         5449         5452         5454         5455 
## 0.8705595394 0.9653546518 0.1641657905 0.2016589056 0.9481644019 0.0601856002 
##         5462         5466         5472         5473         5474         5483 
## 0.7925642035 0.0986079127 0.8982539127 0.6298345333 0.9674084270 0.4808372920 
##         5491         5495         5503         5508         5510         5511 
## 0.1853057139 0.8901305720 0.5004726125 0.1290207360 0.8726360499 0.5202757953 
##         5513         5515         5520         5524         5529         5539 
## 0.0498392339 0.0308041601 0.9484663259 0.6402121183 0.7875999894 0.2195308723 
##         5545         5550         5551         5554         5566         5584 
## 0.0596494962 0.0983445653 0.1591324578 0.6620892682 0.3986204033 0.0090319210 
##         5589         5599         5602         5604         5607         5608 
## 0.3657876167 0.1603072196 0.9959962344 0.3824281895 0.8470823746 0.9095675936 
##         5614         5616         5621         5622         5628         5638 
## 0.3035668499 0.2246701658 0.3167686787 0.9160510623 0.0788755518 0.4842219228 
##         5642         5651         5654         5662         5666         5675 
## 0.8595467139 0.0941071135 0.2619326892 0.2380027262 0.0426382511 0.0976039446 
##         5676         5691         5693         5704         5707         5709 
## 0.9841528897 0.1124614101 0.8328841375 0.5122996502 0.0970374135 0.9819160700 
##         5710         5715         5718         5720         5723         5726 
## 0.1096591979 0.4929192813 0.0252030760 0.7699172349 0.1563505357 0.9810987540 
##         5729         5732         5736         5739         5740         5745 
## 0.4585605105 0.5664395408 0.1986277213 0.1318253491 0.2849459147 0.5038176318 
##         5746         5751         5752         5757         5758         5759 
## 0.9060147756 0.3026174369 0.6916912063 0.1560227384 0.6536874818 0.0272457001 
##         5765         5770         5772         5777         5779         5780 
## 0.0989942450 0.3914544908 0.9309691954 0.1586885608 0.5309921505 0.1846826242 
##         5782         5797         5806         5810         5811         5814 
## 0.2578355382 0.9732447830 0.1537003261 0.2918518995 0.8959532306 0.4264892039 
##         5820         5828         5835         5849         5857         5862 
## 0.1378817860 0.1228178948 0.6639213244 0.6305886094 0.9593788201 0.0428873220 
##         5865         5867         5868         5871         5875         5878 
## 0.2993424229 0.5239602136 0.1734658432 0.9300815772 0.2424716299 0.2792958835 
##         5881         5882         5887         5893         5901         5904 
## 0.4569576088 0.1430461595 0.4020832685 0.8539205327 0.0402830364 0.1915323077 
##         5909         5911         5922         5923         5924         5925 
## 0.1604089008 0.0338109359 0.0326276827 0.0017919050 0.9512286728 0.0722594846 
##         5930         5931         5932         5934         5937         5941 
## 0.2136774227 0.8856957162 0.9332830988 0.1201137182 0.6626551788 0.3606996585 
##         5946         5949         5951         5953         5963         5973 
## 0.9915768093 0.5409876487 0.9772078022 0.4832150471 0.0244964110 0.0511865982 
##         5977         5978         5980         5984         5993         5995 
## 0.1680903349 0.8924745402 0.9731706225 0.5891310126 0.1324158674 0.6636704305 
##         5998         6000         6004         6008         6015         6016 
## 0.5322730939 0.2920653840 0.2378149814 0.0381094670 0.6695388059 0.7982738384 
##         6018         6030         6035         6044         6047         6049 
## 0.5580779224 0.0621572149 0.3319712645 0.9326326524 0.4453703588 0.0038999565 
##         6050         6052         6056         6057         6059         6063 
## 0.7911144149 0.5107677762 0.0171563365 0.2218190615 0.4969995118 0.3730819017 
##         6065         6066         6068         6069         6076         6096 
## 0.3378421979 0.1676492517 0.0519055809 0.3475178125 0.4993513265 0.5610380060 
##         6097         6099         6100         6103         6106         6109 
## 0.2020955609 0.5360540001 0.8381384776 0.5978917880 0.8321811425 0.5334258518 
##         6115         6116         6117         6119         6121         6125 
## 0.0776338466 0.1115972219 0.1659276751 0.2201545231 0.0185363324 0.8936276194 
##         6129         6142         6143         6146         6148         6158 
## 0.8947526646 0.2000381556 0.0449770094 0.4899273713 0.0497853716 0.1791584686 
##         6159         6167         6168         6169         6177         6185 
## 0.9618058653 0.3549463340 0.1689864571 0.7171206696 0.3822016750 0.9747337265 
##         6190         6193         6196         6199         6200         6206 
## 0.5117456149 0.5345687581 0.2805568139 0.6357565949 0.6154613986 0.9059392007 
##         6211         6219         6221         6222         6223         6224 
## 0.1975068649 0.9597540987 0.8407475901 0.1206905493 0.5681185945 0.2715846224 
##         6228         6235         6237         6242         6246         6250 
## 0.2656286487 0.1715638734 0.6210097672 0.0206886534 0.2713821656 0.0315542813 
##         6258         6264         6265         6268         6272         6275 
## 0.0850517128 0.2569796651 0.1268491103 0.0417794271 0.8843360209 0.2104757556 
##         6284         6285         6290         6293         6294         6296 
## 0.4248007505 0.4429775364 0.1307723055 0.0958076620 0.3469932174 0.2944125834 
##         6306         6317         6324         6325         6328         6334 
## 0.9614601451 0.1042474296 0.9645641673 0.7623110548 0.2040538364 0.3070560153 
##         6337         6340         6344         6348         6350         6352 
## 0.2225916747 0.2392021840 0.0033041793 0.1498003868 0.4330713602 0.1264324690 
##         6353         6354         6358         6360         6361         6362 
## 0.2934996954 0.8684607776 0.2503956603 0.8177809220 0.0231529730 0.8798898619 
##         6363         6379         6380         6391         6393         6397 
## 0.5860313756 0.2760761287 0.1361387624 0.3115374577 0.0508197580 0.6363852972 
##         6400         6411         6415         6418         6419         6422 
## 0.4551267958 0.9708876382 0.4221543476 0.2508128883 0.1840297345 0.4967950799 
##         6423         6430         6437         6440         6444         6451 
## 0.9390255872 0.2469377378 0.6463982496 0.4377681670 0.3370168231 0.5179222936 
##         6460         6462         6463         6465         6466         6467 
## 0.0273777069 0.1001022998 0.0204723756 0.5691295353 0.0500110292 0.6059019547 
##         6469         6471         6477         6478         6481         6483 
## 0.9432541608 0.2881662452 0.9079357698 0.0229290844 0.9177695999 0.2039581787 
##         6485         6486         6487         6488         6489         6490 
## 0.0490907065 0.0246059729 0.9983559968 0.0912424927 0.0265131983 0.1204492218 
##         6491         6492         6497         6499         6502         6508 
## 0.3270515323 0.1396073656 0.0474876002 0.1939831518 0.6780246347 0.1790118827 
##         6515         6521         6522         6527         6532         6534 
## 0.5173334940 0.6429027788 0.0708476650 0.9324213454 0.5946933828 0.0822442796 
##         6535         6538         6549         6551         6553         6555 
## 0.4766924316 0.6050499419 0.9677271937 0.0748165543 0.5895881206 0.3938677447 
##         6559         6561         6563         6565         6566         6569 
## 0.3222477819 0.9501677747 0.2035495722 0.9264531584 0.2895063933 0.4661301330 
##         6575         6580         6582         6586         6614         6615 
## 0.2824156580 0.8362934989 0.8615650168 0.1172630087 0.4068899470 0.6065829045 
##         6617         6618         6620         6632         6646         6654 
## 0.0821475307 0.3351175201 0.4067171621 0.0508315050 0.3138171245 0.1626492031 
##         6658         6661         6663         6664         6665         6673 
## 0.1508890623 0.1761548909 0.3134423932 0.8031785966 0.4640561162 0.0980603991 
##         6677         6681         6682         6686         6687         6690 
## 0.0825642143 0.8227168676 0.5939480955 0.5123273551 0.1687204679 0.6969819270 
##         6693         6694         6695         6697         6698         6702 
## 0.6670526336 0.3716476585 0.7361070328 0.6636851035 0.0710496947 0.1003426265 
##         6705         6717         6724         6727         6728         6730 
## 0.9595189831 0.8723037830 0.6300202336 0.0078000850 0.2970316902 0.7050377265 
##         6732         6733         6738         6740         6743         6751 
## 0.9888168305 0.2736542739 0.9877644076 0.4896425017 0.9389490055 0.1562002255 
##         6756         6757         6760         6763         6767         6770 
## 0.3172600729 0.0408282631 0.6099494678 0.8208625751 0.0572388170 0.0469243319 
##         6771         6777         6782         6783         6784         6788 
## 0.0380327749 0.8804977662 0.1031505485 0.3354008434 0.2456969363 0.2540935272 
##         6790         6793         6797         6798         6803         6807 
## 0.0460549694 0.6199676407 0.0656970219 0.0405781145 0.9193567393 0.0540881163 
##         6813         6815         6825         6832         6837         6848 
## 0.9487519657 0.1332157296 0.0488258895 0.5785278880 0.9589263561 0.0210270238 
##         6850         6852         6855         6859         6864         6868 
## 0.2538128427 0.0331246680 0.1551765829 0.0220730344 0.7793939296 0.0282473760 
##         6871         6873         6885         6887         6891         6895 
## 0.7496984225 0.9061561056 0.4444788976 0.0285568599 0.3926877320 0.9578586860 
##         6900         6903         6909         6920         6922         6926 
## 0.4827418702 0.4563419942 0.9496729034 0.2762986818 0.1529105968 0.3482972484 
##         6930         6932         6941         6952         6953         6958 
## 0.0455206511 0.2689672941 0.3439518771 0.4171572020 0.0602906824 0.6100458944 
##         6961         6964         6974         6976         6977         6978 
## 0.2718034761 0.1955415828 0.1053211143 0.2001991133 0.0401679076 0.0806522942 
##         6979         6981         6986         6993         7002         7005 
## 0.4376456407 0.9999985915 0.0554916172 0.9062567930 0.0690795954 0.9160852550 
##         7015         7020         7022         7026         7028         7039 
## 0.2345674649 0.8772176257 0.3888451538 0.4743550929 0.1471233025 0.3146291306 
##         7043         7047         7058         7073         7074         7081 
## 0.0181511339 0.3583186914 0.6454518834 0.2602659241 0.9826379471 0.3161787773 
##         7083         7084         7085         7086         7087         7089 
## 0.2757405495 0.0006488195 0.4130511215 0.5324344267 0.2539583922 0.9894953045 
##         7094         7097         7098         7104         7110         7115 
## 0.4176124354 0.1743528454 0.3463012718 0.0683841403 0.4933701363 0.9700878794 
##         7118         7124         7129         7133         7138         7146 
## 0.5328560793 0.0282403528 0.6041963066 0.8739758692 0.3452842674 0.0662225348 
##         7155         7160         7164         7165         7169         7172 
## 0.1303507373 0.1705661866 0.6416392054 0.1627737666 0.9017530777 0.0519841815 
##         7173         7180         7187         7194         7196         7200 
## 0.2818476219 0.5939577047 0.0292057501 0.2617446472 0.5755498632 0.0605195640 
##         7202         7203         7204         7211         7215         7217 
## 0.9893921661 0.0707037524 0.0276416881 0.0106452920 0.7469952659 0.1405979854 
##         7220         7226         7237         7240         7241         7244 
## 0.1426199013 0.2121144578 0.7304083883 0.9891814556 0.4592619060 0.0219609608 
##         7248         7249         7254         7263         7264         7265 
## 0.3687419188 0.5315868393 0.8449246971 0.8465078567 0.1125391138 0.1041187081 
##         7267         7268         7271         7273         7274         7277 
## 0.4393763170 0.1248833675 0.7041436878 0.2161027400 0.0182300999 0.2253392140 
##         7278         7283         7289         7295         7298         7300 
## 0.9158116415 0.1953998257 0.1467724007 0.3061117763 0.9058356851 0.0479137423 
##         7311         7322         7325         7326         7327         7332 
## 0.0924853319 0.8513873863 0.6629117540 0.3134090408 0.3119370607 0.4523286347 
##         7337         7343         7344         7345         7347         7348 
## 0.4678011762 0.8138901632 0.0298942361 0.6052681547 0.0418168566 0.0487252660 
##         7351         7355         7357         7363         7366         7367 
## 0.1106228809 0.9343116083 0.6331113410 0.4599261419 0.4628384129 0.1345859813 
##         7368         7372         7374         7380         7383         7389 
## 0.2294276931 0.1280525256 0.9765657938 0.6658840603 0.4735053719 0.0242880834 
##         7391         7393         7396         7402         7405         7409 
## 0.0998005500 0.3237764933 0.8965277859 0.0387801821 0.9998441945 0.4258805747 
##         7415         7418         7425         7428         7433         7435 
## 0.9149237997 0.3952719411 0.0002403137 0.1031685121 0.9898557583 0.0547637960 
##         7438         7439         7445         7448         7457         7458 
## 0.4354302851 0.4015187434 0.9987105964 0.6685971290 0.1184313153 0.2388010404 
##         7463         7465         7470         7471         7475         7476 
## 0.0968075738 0.6861141918 0.2162114591 0.8330426445 0.2288547026 0.2702654580 
##         7483         7484         7487         7489         7491         7493 
## 0.3235253563 0.2828863881 0.9452929750 0.4970123188 0.0558496021 0.8286141286 
##         7494         7495         7498         7504         7508         7509 
## 0.0451569889 0.2587167266 0.2323183369 0.8976062905 0.0222077239 0.7503541504 
##         7521         7523         7524         7527         7538         7542 
## 0.5377292780 0.8469362104 0.0674959616 0.8682350593 0.0519403742 0.3225453111 
##         7546         7547         7553         7560         7570         7571 
## 0.8999694613 0.6776658114 0.4870976572 0.0337628137 0.1720970684 0.1254086689 
##         7578         7579         7583         7584         7593         7597 
## 0.0636413832 0.5113028981 0.0631477279 0.0843888512 0.5739508256 0.8576748617 
##         7603         7606         7608         7609         7615         7618 
## 0.4099403820 0.6045363422 0.7508515493 0.2637270884 0.1192846811 0.2602935096 
##         7620         7628         7630         7631         7637         7638 
## 0.3565266034 0.9932838249 0.0775581287 0.1668019845 0.2058025371 0.1287848176 
##         7640         7642         7644         7649         7653         7663 
## 0.4187711651 0.5553609135 0.9981640822 0.9267866861 0.2064546939 0.2456954756 
##         7668         7669         7671         7672         7673         7677 
## 0.0883314180 0.3742330307 0.0149279334 0.3957114550 0.6979448057 0.2987606225 
##         7682         7684         7686         7687         7691         7694 
## 0.7456277072 0.4989326852 0.0008342736 0.6614426637 0.5133048029 0.0638651801 
##         7698         7701         7702         7718         7721         7722 
## 0.4094875229 0.0657965908 0.0590783622 0.1084583093 0.1495082916 0.4763462175 
##         7724         7725         7738         7742         7744         7745 
## 0.8978702810 0.2682594897 0.8688226642 0.4563831206 0.5151443783 0.9236938116 
##         7747         7756         7760         7767         7768         7771 
## 0.8424981042 0.5074891877 0.1827605236 0.8869207598 0.0400784824 0.9964577127 
##         7772         7774         7783         7785         7787         7788 
## 0.6335399744 0.4690560497 0.0910954635 0.4311157120 0.3577044239 0.6300555274 
##         7793         7798         7799         7801         7805         7808 
## 0.3043459047 0.1415326779 0.2151965181 0.9960122414 0.5227561525 0.4611415132 
##         7813         7815         7821         7824         7829         7832 
## 0.7222843383 0.3920625751 0.0888341249 0.1338069273 0.1413685419 0.3744242882 
##         7839         7840         7845         7847         7850         7851 
## 0.5421373372 0.8706999455 0.3165516219 0.1307840157 0.6383328054 0.0601839524 
##         7865         7868         7874         7879         7881         7882 
## 0.3853557182 0.2973873778 0.8699408616 0.1874448779 0.8077431044 0.0196950666 
##         7884         7886         7889         7890         7893         7902 
## 0.1603857389 0.0639259751 0.9973125263 0.2894629597 0.1139301658 0.9807547831 
##         7904         7908         7912         7918         7920         7921 
## 0.8716877476 0.1719655162 0.9956960456 0.1443797407 0.1275611158 0.0988324645 
##         7925         7930         7931         7937         7938         7947 
## 0.8977462360 0.0536120114 0.1779234437 0.0070797753 0.1655613212 0.9411788322 
##         7948         7952         7960         7962         7970         7972 
## 0.6248251078 0.0152342986 0.9924921036 0.4359332985 0.0229615611 0.2733962104 
##         7973         7975         7978         7979         7980         7983 
## 0.9835287690 0.3907914587 0.0686369665 0.4530486333 0.9908278586 0.9898470761 
##         7985         7987         7988         7991         8000         8002 
## 0.0464152181 0.9193874110 0.8620981454 0.9977932331 0.1100622464 0.9915733909 
##         8015         8019         8021         8025         8033         8036 
## 0.9878421703 0.2700104305 0.1343124572 0.6153289732 0.6945166781 0.2177702256 
##         8045         8051         8052         8056         8057         8059 
## 0.7088399932 0.9988378599 0.2001589371 0.3485668957 0.9061435809 0.4990304442 
##         8063         8067         8078         8079         8080         8083 
## 0.8013775106 0.4269293624 0.8659621366 0.1064679197 0.0903844865 0.1072217465 
##         8087         8091         8096         8099         8104         8108 
## 0.0955849775 0.0623618265 0.1413721925 0.1024613990 0.1139896321 0.3008028491 
##         8109         8112         8124         8136         8138         8140 
## 0.9971768410 0.0629836559 0.1672086410 0.0422126014 0.3695951458 0.9438498950 
##         8142         8144         8147         8149         8153         8154 
## 0.0338398491 0.1023026847 0.4908054877 0.1791693125 0.3574532760 0.9308677143 
##         8157         8160         8170         8176         8183         8206 
## 0.9876275644 0.4116433363 0.2186167063 0.8168667266 0.9931355052 0.3483357037 
##         8207         8209         8211         8216         8230         8241 
## 0.4279646482 0.3264669900 0.0851804847 0.2721992022 0.3469934901 0.0690838069 
##         8245         8249         8255         8265         8266         8267 
## 0.1369898711 0.2762762765 0.1515746773 0.0001645700 0.1251792783 0.9458550007 
##         8273         8284         8285         8289         8294         8297 
## 0.9594486889 0.4806863704 0.7816911580 0.3610272184 0.2071575643 0.0285060588 
##         8303         8305         8319         8320         8322         8325 
## 0.0679959682 0.6294381900 0.9420492102 0.4661849021 0.9099086629 0.0390349603 
##         8330         8335         8339         8340         8341         8349 
## 0.6503877532 0.5615915068 0.0740465081 0.6844706395 0.7804705436 0.1737888669 
##         8357         8358         8366         8371         8372         8373 
## 0.7203391739 0.0643430160 0.2075303929 0.0363743900 0.3296286250 0.1601625074 
##         8374         8386         8389         8396         8399         8412 
## 0.6786673083 0.0565757117 0.1994286309 0.0440869935 0.1948840740 0.2084650583 
##         8413         8425         8429         8434         8438         8440 
## 0.1497561978 0.1381987496 0.7798062316 0.9925426483 0.7790550780 0.9303494516 
##         8446         8451         8453         8454         8462         8464 
## 0.0412895406 0.6652936830 0.1301928336 0.9999434721 0.2246600373 0.5193171953 
##         8469         8472         8473         8479         8481         8487 
## 0.2735949964 0.3979867737 0.4832977107 0.5170726567 0.4896236159 0.0811328246 
##         8491         8494         8499         8501         8504         8514 
## 0.0389597954 0.5144790599 0.4742996031 0.0254169161 0.7551348045 0.0820815406 
##         8521         8526 
## 0.6908998166 0.3197976914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y_pred =}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(prob_pred }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{y_pred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    2    5    6    7   11   15   16   18   20   21   31   36   37   41   42   43 
##    1    1    1    0    0    0    0    0    1    0    1    1    1    0    0    1 
##   56   59   63   64   67   69   83   86   89  101  110  115  124  125  128  140 
##    1    0    1    0    0    0    0    0    0    1    0    0    1    0    0    0 
##  143  148  149  151  154  155  158  160  163  164  165  167  169  177  179  182 
##    0    0    1    0    1    0    1    0    1    0    0    0    1    0    1    0 
##  183  187  194  197  201  202  205  211  217  218  220  225  226  229  236  245 
##    0    0    0    0    0    0    0    1    1    1    0    0    0    0    0    0 
##  247  252  254  256  261  264  267  270  274  279  283  290  291  293  298  301 
##    0    0    0    0    0    0    0    1    0    1    0    0    0    0    0    0 
##  303  314  321  322  323  324  326  334  339  341  346  349  350  351  353  365 
##    1    1    1    0    0    1    0    0    0    0    0    0    0    0    0    0 
##  366  367  371  374  383  385  389  397  399  404  405  406  407  413  414  420 
##    0    0    0    0    0    0    1    0    1    0    1    1    0    0    1    1 
##  421  422  438  439  445  446  449  450  456  464  468  474  476  481  485  499 
##    1    0    1    1    0    0    1    0    1    1    1    1    0    1    1    0 
##  500  506  509  512  513  520  524  528  531  532  535  537  540  551  552  554 
##    1    1    1    0    1    0    0    0    1    0    0    0    1    1    0    1 
##  555  559  560  570  573  574  587  589  598  599  603  604  605  607  608  616 
##    0    0    0    0    1    1    0    0    1    1    0    0    0    1    1    1 
##  622  626  633  634  637  638  649  654  655  656  657  662  666  667  668  669 
##    1    1    0    0    1    0    0    0    0    0    0    0    1    0    1    1 
##  673  674  675  676  682  683  685  689  692  705  711  717  719  721  725  726 
##    0    0    0    1    1    0    1    0    0    0    0    0    1    1    0    1 
##  727  729  731  734  737  741  745  751  756  762  766  768  774  775  778  780 
##    0    1    0    0    0    0    1    0    1    0    0    0    1    1    1    1 
##  784  788  794  796  797  802  803  808  816  818  821  822  827  829  832  838 
##    0    0    0    0    1    0    0    0    1    1    0    0    0    0    1    0 
##  842  848  849  860  861  865  870  876  878  880  881  887  888  892  893  900 
##    1    0    0    0    0    1    1    0    0    0    0    0    1    1    0    0 
##  901  907  913  915  919  922  923  925  927  931  936  944  951  953  954  958 
##    1    0    0    0    1    0    0    0    0    0    1    0    0    0    0    1 
##  959  960  962  964  967  971  982  998  999 1003 1008 1013 1026 1028 1034 1042 
##    0    1    0    1    1    0    0    0    0    1    0    1    0    0    0    0 
## 1047 1049 1050 1056 1062 1069 1070 1081 1085 1089 1093 1096 1111 1119 1129 1131 
##    0    1    1    1    0    0    0    0    0    0    1    1    1    0    1    0 
## 1133 1134 1138 1140 1142 1146 1147 1148 1153 1156 1162 1167 1170 1175 1176 1182 
##    0    0    0    1    0    0    0    0    0    0    1    0    0    0    0    0 
## 1185 1193 1198 1202 1213 1214 1225 1232 1240 1241 1244 1249 1252 1257 1262 1266 
##    0    0    0    0    1    1    0    1    0    0    0    0    1    1    0    0 
## 1269 1270 1278 1284 1287 1290 1294 1303 1305 1313 1318 1319 1329 1330 1335 1338 
##    0    1    0    0    0    1    0    0    1    1    1    0    1    0    0    1 
## 1339 1341 1344 1347 1349 1356 1357 1364 1367 1368 1373 1374 1375 1379 1383 1385 
##    0    1    0    0    1    0    0    0    1    1    0    0    0    1    1    0 
## 1389 1396 1404 1406 1407 1408 1410 1412 1419 1421 1423 1427 1428 1429 1432 1436 
##    1    1    0    0    0    1    0    1    0    1    0    1    0    1    0    1 
## 1443 1446 1451 1453 1457 1463 1466 1473 1475 1476 1477 1478 1479 1481 1483 1484 
##    0    0    1    1    0    0    1    0    1    1    0    0    0    1    1    1 
## 1485 1487 1492 1496 1499 1501 1507 1508 1512 1516 1520 1523 1527 1529 1533 1546 
##    0    0    1    0    0    1    0    0    1    1    1    1    1    0    1    0 
## 1547 1548 1552 1553 1558 1561 1565 1569 1571 1577 1580 1581 1599 1619 1621 1623 
##    0    1    0    0    0    1    0    1    0    0    1    1    0    0    1    0 
## 1634 1638 1640 1641 1645 1652 1665 1673 1677 1683 1689 1694 1718 1719 1723 1727 
##    1    0    0    0    0    0    1    0    1    0    0    0    1    0    0    0 
## 1729 1730 1731 1735 1736 1737 1740 1743 1746 1751 1752 1754 1760 1761 1763 1767 
##    1    0    0    1    0    1    0    0    1    1    0    0    0    1    0    1 
## 1769 1782 1783 1792 1796 1803 1806 1807 1808 1816 1821 1823 1824 1827 1830 1837 
##    0    0    0    0    0    0    1    1    1    1    0    0    0    0    0    0 
## 1839 1840 1843 1846 1849 1855 1856 1858 1864 1866 1870 1872 1876 1878 1882 1891 
##    1    0    0    0    1    0    0    0    0    0    0    1    1    0    1    0 
## 1893 1900 1904 1925 1926 1928 1929 1933 1939 1943 1956 1964 1966 1972 1975 1976 
##    1    1    1    0    1    1    1    1    1    1    1    0    0    1    0    0 
## 1981 1991 1999 2003 2004 2007 2008 2015 2017 2018 2020 2027 2028 2029 2035 2042 
##    0    0    0    0    0    1    0    0    0    0    1    0    1    0    1    0 
## 2046 2049 2050 2052 2053 2056 2073 2074 2076 2081 2083 2084 2088 2093 2098 2102 
##    1    0    0    0    0    0    0    0    1    0    1    0    0    0    0    0 
## 2107 2113 2115 2117 2120 2134 2135 2139 2140 2145 2150 2152 2154 2155 2162 2167 
##    1    1    0    0    0    0    1    0    1    1    0    0    0    1    0    1 
## 2168 2174 2175 2188 2198 2199 2203 2216 2217 2218 2222 2226 2230 2235 2241 2247 
##    0    1    1    0    0    0    0    0    1    0    0    0    1    0    1    0 
## 2253 2254 2255 2256 2259 2264 2270 2272 2275 2277 2278 2282 2283 2289 2309 2315 
##    1    1    0    0    1    1    0    1    0    0    0    0    0    0    1    0 
## 2316 2322 2327 2329 2330 2332 2338 2345 2349 2353 2356 2357 2365 2373 2376 2377 
##    1    1    0    0    0    0    0    1    0    0    1    0    0    0    0    0 
## 2385 2389 2391 2392 2394 2400 2403 2406 2408 2412 2421 2426 2430 2431 2432 2436 
##    1    0    1    1    0    0    0    0    1    0    1    0    0    0    1    1 
## 2439 2447 2457 2460 2462 2464 2465 2466 2469 2474 2475 2479 2486 2488 2491 2504 
##    0    0    1    1    0    0    0    0    0    0    0    1    1    0    0    0 
## 2505 2510 2514 2515 2516 2520 2521 2523 2526 2531 2534 2535 2537 2538 2541 2542 
##    0    1    0    1    0    0    1    0    0    0    1    1    0    1    0    0 
## 2546 2554 2563 2574 2579 2580 2581 2582 2586 2591 2594 2595 2598 2601 2610 2615 
##    0    0    1    1    1    0    0    0    0    0    1    1    0    0    0    0 
## 2619 2626 2627 2631 2632 2639 2640 2644 2645 2647 2649 2656 2657 2658 2659 2661 
##    0    0    0    0    1    0    0    0    1    0    1    0    1    0    0    0 
## 2662 2669 2672 2675 2679 2688 2693 2695 2696 2699 2703 2704 2712 2722 2736 2744 
##    0    0    0    1    0    0    0    0    0    1    1    1    0    0    0    1 
## 2746 2747 2751 2753 2754 2755 2758 2763 2767 2768 2769 2770 2771 2780 2784 2785 
##    1    1    0    0    0    1    0    0    0    1    0    0    0    1    0    1 
## 2787 2793 2798 2803 2804 2807 2810 2812 2820 2825 2832 2834 2839 2843 2848 2850 
##    0    0    1    1    1    0    0    0    0    0    0    0    0    0    0    1 
## 2856 2866 2872 2873 2878 2879 2883 2892 2899 2900 2902 2905 2906 2915 2916 2923 
##    0    0    0    0    1    1    0    0    1    0    0    1    0    1    1    0 
## 2927 2934 2936 2940 2943 2945 2949 2951 2957 2959 2962 2963 2965 2967 2981 2984 
##    1    1    0    1    1    0    0    1    0    0    1    0    0    0    1    0 
## 2985 2989 3004 3011 3017 3028 3037 3042 3044 3050 3058 3059 3061 3064 3067 3068 
##    0    0    1    0    1    1    1    0    1    0    0    0    0    0    0    1 
## 3071 3074 3075 3076 3082 3089 3093 3095 3098 3101 3102 3105 3106 3111 3119 3127 
##    1    0    0    0    1    0    0    0    0    0    1    0    1    0    1    0 
## 3134 3136 3137 3147 3151 3160 3172 3173 3180 3181 3185 3186 3189 3193 3196 3198 
##    0    0    1    0    1    0    1    0    0    1    0    0    0    0    0    1 
## 3204 3206 3210 3212 3217 3227 3231 3233 3238 3240 3243 3245 3246 3249 3254 3257 
##    1    0    0    0    0    0    0    0    1    1    0    0    0    0    0    0 
## 3259 3265 3272 3282 3285 3289 3294 3305 3310 3323 3325 3328 3329 3330 3339 3341 
##    1    0    0    0    1    0    1    0    0    0    0    0    0    1    1    0 
## 3344 3348 3350 3354 3356 3357 3359 3363 3364 3374 3384 3387 3395 3399 3402 3405 
##    0    0    1    0    0    1    0    0    1    1    0    0    0    0    0    1 
## 3408 3409 3416 3417 3419 3427 3430 3431 3432 3434 3435 3438 3441 3450 3461 3462 
##    0    0    0    1    0    1    0    0    0    0    0    1    1    0    1    1 
## 3464 3466 3467 3477 3478 3483 3490 3493 3499 3503 3506 3510 3514 3523 3524 3533 
##    1    1    1    1    0    1    0    0    0    1    0    0    0    0    0    1 
## 3534 3541 3542 3544 3545 3548 3549 3551 3558 3559 3563 3579 3580 3583 3585 3587 
##    0    1    0    0    1    0    1    1    1    0    0    1    1    0    1    1 
## 3590 3591 3592 3596 3597 3598 3602 3603 3608 3610 3613 3620 3621 3624 3626 3630 
##    1    0    1    1    0    0    1    1    1    1    1    0    0    0    0    1 
## 3631 3641 3646 3647 3650 3654 3659 3660 3664 3676 3689 3692 3695 3700 3706 3707 
##    0    0    0    0    0    0    1    1    0    0    1    0    1    0    1    1 
## 3711 3714 3715 3724 3726 3727 3741 3742 3749 3755 3756 3758 3761 3763 3766 3775 
##    1    0    1    0    1    1    0    0    0    1    0    0    0    0    1    0 
## 3778 3779 3781 3782 3789 3795 3801 3808 3819 3823 3824 3833 3836 3839 3841 3846 
##    1    1    1    1    0    1    0    0    0    0    1    0    0    1    0    1 
## 3847 3851 3872 3877 3878 3881 3882 3883 3893 3895 3896 3898 3899 3900 3906 3914 
##    0    0    1    1    1    0    1    0    0    0    1    0    0    1    0    0 
## 3917 3924 3927 3928 3937 3938 3941 3949 3951 3952 3958 3960 3962 3963 3965 3967 
##    0    1    0    1    1    0    0    0    1    0    0    0    0    1    0    0 
## 3969 3970 3972 3973 3974 3981 3982 3993 3996 3997 4002 4004 4006 4007 4015 4017 
##    0    0    1    1    0    0    0    0    0    0    1    0    1    0    0    0 
## 4018 4021 4033 4034 4047 4048 4056 4060 4065 4066 4069 4075 4076 4077 4078 4083 
##    1    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0 
## 4088 4090 4092 4095 4097 4100 4108 4109 4110 4112 4113 4122 4127 4131 4134 4135 
##    0    1    0    0    0    0    0    0    1    1    0    0    1    0    0    0 
## 4136 4138 4139 4140 4150 4151 4152 4157 4158 4162 4166 4167 4169 4171 4175 4178 
##    1    0    0    0    0    0    0    0    0    0    0    0    1    0    1    1 
## 4184 4185 4190 4191 4197 4204 4205 4208 4209 4210 4212 4215 4217 4218 4219 4225 
##    0    0    0    1    0    0    0    0    0    0    0    0    0    1    1    1 
## 4236 4238 4241 4243 4245 4247 4248 4249 4256 4259 4266 4269 4271 4280 4282 4283 
##    1    1    1    0    1    1    0    0    0    1    0    0    1    1    1    0 
## 4287 4290 4291 4293 4317 4326 4329 4333 4334 4337 4340 4345 4353 4359 4364 4366 
##    0    1    0    1    0    0    0    0    0    0    0    0    1    0    0    1 
## 4371 4374 4377 4378 4379 4381 4388 4390 4391 4396 4403 4407 4409 4413 4415 4417 
##    0    1    0    0    1    0    1    0    0    0    0    0    1    1    0    0 
## 4419 4423 4424 4427 4428 4431 4433 4438 4439 4440 4444 4445 4448 4456 4461 4464 
##    1    1    1    0    0    0    0    0    0    0    1    1    0    0    1    0 
## 4469 4479 4481 4482 4484 4485 4486 4489 4492 4498 4502 4505 4507 4509 4510 4512 
##    0    0    0    0    1    0    0    0    0    0    0    0    0    0    1    0 
## 4517 4520 4523 4524 4526 4533 4534 4540 4542 4548 4552 4560 4572 4578 4582 4585 
##    1    0    1    0    0    0    1    1    1    0    0    1    1    1    1    0 
## 4596 4625 4630 4633 4642 4654 4656 4670 4674 4678 4688 4693 4698 4703 4715 4716 
##    1    0    1    0    1    0    0    0    1    1    1    0    1    1    0    1 
## 4722 4723 4731 4739 4742 4746 4748 4753 4757 4759 4763 4771 4775 4778 4780 4783 
##    0    1    0    0    1    1    1    1    0    0    0    1    0    0    0    0 
## 4784 4786 4787 4788 4801 4806 4807 4811 4817 4826 4831 4840 4845 4854 4858 4863 
##    1    0    0    0    1    1    0    1    0    0    0    1    1    0    1    1 
## 4868 4870 4871 4877 4879 4889 4890 4891 4899 4902 4905 4906 4908 4913 4917 4919 
##    0    0    1    1    0    0    0    0    1    0    0    0    0    0    0    0 
## 4924 4925 4926 4927 4935 4936 4937 4938 4939 4940 4946 4947 4950 4959 4965 4966 
##    1    0    0    0    0    1    0    0    0    1    0    0    0    0    0    1 
## 4968 4970 4972 4978 4980 4984 4987 4988 4989 4991 5000 5003 5006 5009 5011 5014 
##    0    0    0    1    0    1    0    0    1    0    0    0    0    0    1    0 
## 5016 5017 5020 5021 5029 5039 5041 5051 5056 5059 5064 5070 5074 5075 5087 5089 
##    1    0    1    0    1    0    0    0    1    1    0    1    1    1    0    0 
## 5093 5095 5100 5101 5116 5123 5124 5127 5131 5133 5134 5137 5138 5140 5144 5153 
##    0    0    1    1    1    1    0    0    1    1    0    1    1    1    0    0 
## 5154 5163 5166 5172 5186 5188 5189 5193 5194 5197 5204 5206 5211 5214 5215 5216 
##    1    0    0    0    1    0    0    1    0    0    0    1    1    0    0    0 
## 5218 5226 5227 5230 5236 5244 5245 5248 5250 5251 5254 5261 5262 5264 5265 5268 
##    1    0    1    1    0    1    0    0    0    0    0    0    1    1    1    0 
## 5269 5273 5275 5276 5280 5283 5284 5294 5295 5296 5300 5303 5305 5310 5330 5335 
##    0    0    0    0    1    1    1    0    0    1    0    0    1    1    0    0 
## 5352 5353 5355 5357 5358 5359 5360 5363 5365 5366 5369 5371 5376 5378 5382 5385 
##    0    0    1    0    1    1    0    0    1    0    0    0    0    1    0    0 
## 5389 5393 5400 5402 5405 5406 5407 5410 5411 5417 5418 5419 5431 5432 5433 5442 
##    1    0    1    0    1    0    1    0    0    0    1    1    0    1    1    1 
## 5449 5452 5454 5455 5462 5466 5472 5473 5474 5483 5491 5495 5503 5508 5510 5511 
##    0    0    1    0    1    0    1    1    1    0    0    1    1    0    1    1 
## 5513 5515 5520 5524 5529 5539 5545 5550 5551 5554 5566 5584 5589 5599 5602 5604 
##    0    0    1    1    1    0    0    0    0    1    0    0    0    0    1    0 
## 5607 5608 5614 5616 5621 5622 5628 5638 5642 5651 5654 5662 5666 5675 5676 5691 
##    1    1    0    0    0    1    0    0    1    0    0    0    0    0    1    0 
## 5693 5704 5707 5709 5710 5715 5718 5720 5723 5726 5729 5732 5736 5739 5740 5745 
##    1    1    0    1    0    0    0    1    0    1    0    1    0    0    0    1 
## 5746 5751 5752 5757 5758 5759 5765 5770 5772 5777 5779 5780 5782 5797 5806 5810 
##    1    0    1    0    1    0    0    0    1    0    1    0    0    1    0    0 
## 5811 5814 5820 5828 5835 5849 5857 5862 5865 5867 5868 5871 5875 5878 5881 5882 
##    1    0    0    0    1    1    1    0    0    1    0    1    0    0    0    0 
## 5887 5893 5901 5904 5909 5911 5922 5923 5924 5925 5930 5931 5932 5934 5937 5941 
##    0    1    0    0    0    0    0    0    1    0    0    1    1    0    1    0 
## 5946 5949 5951 5953 5963 5973 5977 5978 5980 5984 5993 5995 5998 6000 6004 6008 
##    1    1    1    0    0    0    0    1    1    1    0    1    1    0    0    0 
## 6015 6016 6018 6030 6035 6044 6047 6049 6050 6052 6056 6057 6059 6063 6065 6066 
##    1    1    1    0    0    1    0    0    1    1    0    0    0    0    0    0 
## 6068 6069 6076 6096 6097 6099 6100 6103 6106 6109 6115 6116 6117 6119 6121 6125 
##    0    0    0    1    0    1    1    1    1    1    0    0    0    0    0    1 
## 6129 6142 6143 6146 6148 6158 6159 6167 6168 6169 6177 6185 6190 6193 6196 6199 
##    1    0    0    0    0    0    1    0    0    1    0    1    1    1    0    1 
## 6200 6206 6211 6219 6221 6222 6223 6224 6228 6235 6237 6242 6246 6250 6258 6264 
##    1    1    0    1    1    0    1    0    0    0    1    0    0    0    0    0 
## 6265 6268 6272 6275 6284 6285 6290 6293 6294 6296 6306 6317 6324 6325 6328 6334 
##    0    0    1    0    0    0    0    0    0    0    1    0    1    1    0    0 
## 6337 6340 6344 6348 6350 6352 6353 6354 6358 6360 6361 6362 6363 6379 6380 6391 
##    0    0    0    0    0    0    0    1    0    1    0    1    1    0    0    0 
## 6393 6397 6400 6411 6415 6418 6419 6422 6423 6430 6437 6440 6444 6451 6460 6462 
##    0    1    0    1    0    0    0    0    1    0    1    0    0    1    0    0 
## 6463 6465 6466 6467 6469 6471 6477 6478 6481 6483 6485 6486 6487 6488 6489 6490 
##    0    1    0    1    1    0    1    0    1    0    0    0    1    0    0    0 
## 6491 6492 6497 6499 6502 6508 6515 6521 6522 6527 6532 6534 6535 6538 6549 6551 
##    0    0    0    0    1    0    1    1    0    1    1    0    0    1    1    0 
## 6553 6555 6559 6561 6563 6565 6566 6569 6575 6580 6582 6586 6614 6615 6617 6618 
##    1    0    0    1    0    1    0    0    0    1    1    0    0    1    0    0 
## 6620 6632 6646 6654 6658 6661 6663 6664 6665 6673 6677 6681 6682 6686 6687 6690 
##    0    0    0    0    0    0    0    1    0    0    0    1    1    1    0    1 
## 6693 6694 6695 6697 6698 6702 6705 6717 6724 6727 6728 6730 6732 6733 6738 6740 
##    1    0    1    1    0    0    1    1    1    0    0    1    1    0    1    0 
## 6743 6751 6756 6757 6760 6763 6767 6770 6771 6777 6782 6783 6784 6788 6790 6793 
##    1    0    0    0    1    1    0    0    0    1    0    0    0    0    0    1 
## 6797 6798 6803 6807 6813 6815 6825 6832 6837 6848 6850 6852 6855 6859 6864 6868 
##    0    0    1    0    1    0    0    1    1    0    0    0    0    0    1    0 
## 6871 6873 6885 6887 6891 6895 6900 6903 6909 6920 6922 6926 6930 6932 6941 6952 
##    1    1    0    0    0    1    0    0    1    0    0    0    0    0    0    0 
## 6953 6958 6961 6964 6974 6976 6977 6978 6979 6981 6986 6993 7002 7005 7015 7020 
##    0    1    0    0    0    0    0    0    0    1    0    1    0    1    0    1 
## 7022 7026 7028 7039 7043 7047 7058 7073 7074 7081 7083 7084 7085 7086 7087 7089 
##    0    0    0    0    0    0    1    0    1    0    0    0    0    1    0    1 
## 7094 7097 7098 7104 7110 7115 7118 7124 7129 7133 7138 7146 7155 7160 7164 7165 
##    0    0    0    0    0    1    1    0    1    1    0    0    0    0    1    0 
## 7169 7172 7173 7180 7187 7194 7196 7200 7202 7203 7204 7211 7215 7217 7220 7226 
##    1    0    0    1    0    0    1    0    1    0    0    0    1    0    0    0 
## 7237 7240 7241 7244 7248 7249 7254 7263 7264 7265 7267 7268 7271 7273 7274 7277 
##    1    1    0    0    0    1    1    1    0    0    0    0    1    0    0    0 
## 7278 7283 7289 7295 7298 7300 7311 7322 7325 7326 7327 7332 7337 7343 7344 7345 
##    1    0    0    0    1    0    0    1    1    0    0    0    0    1    0    1 
## 7347 7348 7351 7355 7357 7363 7366 7367 7368 7372 7374 7380 7383 7389 7391 7393 
##    0    0    0    1    1    0    0    0    0    0    1    1    0    0    0    0 
## 7396 7402 7405 7409 7415 7418 7425 7428 7433 7435 7438 7439 7445 7448 7457 7458 
##    1    0    1    0    1    0    0    0    1    0    0    0    1    1    0    0 
## 7463 7465 7470 7471 7475 7476 7483 7484 7487 7489 7491 7493 7494 7495 7498 7504 
##    0    1    0    1    0    0    0    0    1    0    0    1    0    0    0    1 
## 7508 7509 7521 7523 7524 7527 7538 7542 7546 7547 7553 7560 7570 7571 7578 7579 
##    0    1    1    1    0    1    0    0    1    1    0    0    0    0    0    1 
## 7583 7584 7593 7597 7603 7606 7608 7609 7615 7618 7620 7628 7630 7631 7637 7638 
##    0    0    1    1    0    1    1    0    0    0    0    1    0    0    0    0 
## 7640 7642 7644 7649 7653 7663 7668 7669 7671 7672 7673 7677 7682 7684 7686 7687 
##    0    1    1    1    0    0    0    0    0    0    1    0    1    0    0    1 
## 7691 7694 7698 7701 7702 7718 7721 7722 7724 7725 7738 7742 7744 7745 7747 7756 
##    1    0    0    0    0    0    0    0    1    0    1    0    1    1    1    1 
## 7760 7767 7768 7771 7772 7774 7783 7785 7787 7788 7793 7798 7799 7801 7805 7808 
##    0    1    0    1    1    0    0    0    0    1    0    0    0    1    1    0 
## 7813 7815 7821 7824 7829 7832 7839 7840 7845 7847 7850 7851 7865 7868 7874 7879 
##    1    0    0    0    0    0    1    1    0    0    1    0    0    0    1    0 
## 7881 7882 7884 7886 7889 7890 7893 7902 7904 7908 7912 7918 7920 7921 7925 7930 
##    1    0    0    0    1    0    0    1    1    0    1    0    0    0    1    0 
## 7931 7937 7938 7947 7948 7952 7960 7962 7970 7972 7973 7975 7978 7979 7980 7983 
##    0    0    0    1    1    0    1    0    0    0    1    0    0    0    1    1 
## 7985 7987 7988 7991 8000 8002 8015 8019 8021 8025 8033 8036 8045 8051 8052 8056 
##    0    1    1    1    0    1    1    0    0    1    1    0    1    1    0    0 
## 8057 8059 8063 8067 8078 8079 8080 8083 8087 8091 8096 8099 8104 8108 8109 8112 
##    1    0    1    0    1    0    0    0    0    0    0    0    0    0    1    0 
## 8124 8136 8138 8140 8142 8144 8147 8149 8153 8154 8157 8160 8170 8176 8183 8206 
##    0    0    0    1    0    0    0    0    0    1    1    0    0    1    1    0 
## 8207 8209 8211 8216 8230 8241 8245 8249 8255 8265 8266 8267 8273 8284 8285 8289 
##    0    0    0    0    0    0    0    0    0    0    0    1    1    0    1    0 
## 8294 8297 8303 8305 8319 8320 8322 8325 8330 8335 8339 8340 8341 8349 8357 8358 
##    0    0    0    1    1    0    1    0    1    1    0    1    1    0    1    0 
## 8366 8371 8372 8373 8374 8386 8389 8396 8399 8412 8413 8425 8429 8434 8438 8440 
##    0    0    0    0    1    0    0    0    0    0    0    0    1    1    1    1 
## 8446 8451 8453 8454 8462 8464 8469 8472 8473 8479 8481 8487 8491 8494 8499 8501 
##    0    1    0    1    0    1    0    0    0    1    0    0    0    1    0    0 
## 8504 8514 8521 8526 
##    1    0    1    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#making the confusion matrix}
\NormalTok{cm =}\StringTok{ }\KeywordTok{table}\NormalTok{(test_set[,}\DecValTok{17}\NormalTok{], y_pred)}
\NormalTok{cm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    y_pred
##        0    1
##   0 1110  130
##   1  257  635
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\StringTok{"=====================================Logistic Regression====================================="}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "=====================================Logistic Regression====================================="
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(lattice)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{confusionMatrix}\NormalTok{(cm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##    y_pred
##        0    1
##   0 1110  130
##   1  257  635
##                                           
##                Accuracy : 0.8185          
##                  95% CI : (0.8015, 0.8346)
##     No Information Rate : 0.6412          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.6194          
##                                           
##  Mcnemar's Test P-Value : 1.504e-10       
##                                           
##             Sensitivity : 0.8120          
##             Specificity : 0.8301          
##          Pos Pred Value : 0.8952          
##          Neg Pred Value : 0.7119          
##              Prevalence : 0.6412          
##          Detection Rate : 0.5206          
##    Detection Prevalence : 0.5816          
##       Balanced Accuracy : 0.8210          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#=================================================================================================================}
\CommentTok{#Naive Bayes}
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}
\CommentTok{# Importing the dataset}
\NormalTok{dataset =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'train.csv'}\NormalTok{)}

\NormalTok{dataset}\OperatorTok{$}\NormalTok{days_elapsed_old[dataset}\OperatorTok{$}\NormalTok{days_elapsed_old}\OperatorTok{<}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
\NormalTok{dataset[ dataset }\OperatorTok{==}\StringTok{ "na"}\NormalTok{ ] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{#Factor like columns}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{job=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{job))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{marital=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{marital))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{education=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{education))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{device=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{device))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{outcome_old=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{outcome_old))}
\NormalTok{dataset[}\KeywordTok{is.na}\NormalTok{(dataset)] <-}\StringTok{ }\DecValTok{0}

\CommentTok{# Encoding the target feature as factor}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{y =}\StringTok{ }\KeywordTok{factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\CommentTok{#labels /levels -both are same}

\CommentTok{# Splitting the dataset into the Training set and Test set}
\CommentTok{# install.packages('caTools')}
\KeywordTok{library}\NormalTok{(caTools)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{SplitRatio =} \FloatTok{0.75}\NormalTok{)}
\NormalTok{training_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE}\NormalTok{)}
\NormalTok{test_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Feature Scaling}
\NormalTok{training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}
\NormalTok{test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}

\CommentTok{# Fitting Naive Bayes to the Training set}
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{classifier =}\StringTok{ }\KeywordTok{naiveBayes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{],}
                        \DataTypeTok{y =}\NormalTok{ training_set}\OperatorTok{$}\NormalTok{y) }

\CommentTok{# Predicting the Test set results}
\NormalTok{y_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}

\CommentTok{# Making the Confusion Matrix}
\NormalTok{cm =}\StringTok{ }\KeywordTok{table}\NormalTok{(test_set[, }\DecValTok{17}\NormalTok{], y_pred)}

\KeywordTok{print}\NormalTok{(}\StringTok{"=====================================Naive Bayes====================================="}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "=====================================Naive Bayes====================================="
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(lattice)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{confusionMatrix}\NormalTok{(cm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##    y_pred
##        0    1
##   0 1020  220
##   1  325  567
##                                           
##                Accuracy : 0.7444          
##                  95% CI : (0.7253, 0.7628)
##     No Information Rate : 0.6309          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.4659          
##                                           
##  Mcnemar's Test P-Value : 8.394e-06       
##                                           
##             Sensitivity : 0.7584          
##             Specificity : 0.7205          
##          Pos Pred Value : 0.8226          
##          Neg Pred Value : 0.6357          
##              Prevalence : 0.6309          
##          Detection Rate : 0.4784          
##    Detection Prevalence : 0.5816          
##       Balanced Accuracy : 0.7394          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#=================================================================================================================}
\CommentTok{#Decision Tree Classification}

\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}

\CommentTok{# Importing the dataset}
\NormalTok{dataset =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'train.csv'}\NormalTok{)}

\NormalTok{dataset}\OperatorTok{$}\NormalTok{days_elapsed_old[dataset}\OperatorTok{$}\NormalTok{days_elapsed_old}\OperatorTok{<}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
\NormalTok{dataset[ dataset }\OperatorTok{==}\StringTok{ "na"}\NormalTok{ ] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{#Factor like columns}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{job=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{job))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{marital=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{marital))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{education=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{education))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{device=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{device))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{outcome_old=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{outcome_old))}
\NormalTok{dataset[}\KeywordTok{is.na}\NormalTok{(dataset)] <-}\StringTok{ }\DecValTok{0}

\CommentTok{# Encoding the target feature as factor}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{y =}\StringTok{ }\KeywordTok{factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{# Splitting the dataset into the Training set and Test set}
\CommentTok{# install.packages('caTools')}
\KeywordTok{library}\NormalTok{(caTools)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{SplitRatio =} \FloatTok{0.75}\NormalTok{)}
\NormalTok{training_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE}\NormalTok{)}
\NormalTok{test_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Feature Scaling #no need to scale,but to visualise in high resolution if we scale, the results will be fast otherwise code may break}
\NormalTok{training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}
\NormalTok{test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}

\CommentTok{# Fitting Decision TreeClassification to the Training set}
\KeywordTok{library}\NormalTok{(rpart)}
\NormalTok{classifier =}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ y }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                   \DataTypeTok{data =}\NormalTok{ training_set)}

\CommentTok{# Predicting the Test set results}
\NormalTok{y_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{], }\DataTypeTok{type =} \StringTok{'class'}\NormalTok{) }

\CommentTok{# Making the Confusion Matrix}
\NormalTok{cm =}\StringTok{ }\KeywordTok{table}\NormalTok{(test_set[, }\DecValTok{17}\NormalTok{], y_pred)}

\KeywordTok{print}\NormalTok{(}\StringTok{"=====================================Decision Trees====================================="}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "=====================================Decision Trees====================================="
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(lattice)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{confusionMatrix}\NormalTok{(cm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##    y_pred
##        0    1
##   0 1064  176
##   1  262  630
##                                           
##                Accuracy : 0.7946          
##                  95% CI : (0.7768, 0.8115)
##     No Information Rate : 0.622           
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.5721          
##                                           
##  Mcnemar's Test P-Value : 4.877e-05       
##                                           
##             Sensitivity : 0.8024          
##             Specificity : 0.7816          
##          Pos Pred Value : 0.8581          
##          Neg Pred Value : 0.7063          
##              Prevalence : 0.6220          
##          Detection Rate : 0.4991          
##    Detection Prevalence : 0.5816          
##       Balanced Accuracy : 0.7920          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#=================================================================================================================}
\CommentTok{# k-nearest neighbors (K-NN)}

\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}

\CommentTok{# Importing the dataset}
\NormalTok{dataset =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'train.csv'}\NormalTok{)}

\NormalTok{dataset}\OperatorTok{$}\NormalTok{days_elapsed_old[dataset}\OperatorTok{$}\NormalTok{days_elapsed_old}\OperatorTok{<}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
\NormalTok{dataset[ dataset }\OperatorTok{==}\StringTok{ "na"}\NormalTok{ ] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{#Factor like columns}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{job=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{job))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{marital=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{marital))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{education=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{education))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{device=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{device))}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{outcome_old=}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{outcome_old))}
\NormalTok{dataset[}\KeywordTok{is.na}\NormalTok{(dataset)] <-}\StringTok{ }\DecValTok{0}

\CommentTok{# Encoding the target feature as factor}
\CommentTok{# Encoding the target feature as factor #(the values are considered as numeric values i.e 1 > 0 but we don't want that. }
\CommentTok{#Instead we want them to consider as factors i.e 1 and 0 as two different categories.)}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{y =}\StringTok{ }\KeywordTok{factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{# Splitting the dataset into the Training set and Test set}
\CommentTok{# install.packages('caTools')}
\KeywordTok{library}\NormalTok{(caTools)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{SplitRatio =} \FloatTok{0.75}\NormalTok{)}
\NormalTok{training_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE}\NormalTok{)}
\NormalTok{test_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Feature Scaling}
\NormalTok{training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(training_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}
\NormalTok{test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(test_set[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}

\CommentTok{# Fitting K-NN to the Training set and predicting the test set results}
\CommentTok{#install.packages('class')}
\KeywordTok{library}\NormalTok{(class)}
\NormalTok{y_pred =}\StringTok{ }\KeywordTok{knn}\NormalTok{(}\DataTypeTok{train =}\NormalTok{ training_set[, }\DecValTok{-17}\NormalTok{],}
             \DataTypeTok{test =}\NormalTok{ test_set[, }\DecValTok{-17}\NormalTok{],}
             \DataTypeTok{cl =}\NormalTok{ training_set[, }\DecValTok{17}\NormalTok{],}\DataTypeTok{k =} \DecValTok{20}\NormalTok{)}
\NormalTok{y_pred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    [1] 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1
##   [38] 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1
##   [75] 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1
##  [112] 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0
##  [149] 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0
##  [186] 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0
##  [223] 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0
##  [260] 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1
##  [297] 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1
##  [334] 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0
##  [371] 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0
##  [408] 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1
##  [445] 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1
##  [482] 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0
##  [519] 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0
##  [556] 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0
##  [593] 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0
##  [630] 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0
##  [667] 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0
##  [704] 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0
##  [741] 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0
##  [778] 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0
##  [815] 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0
##  [852] 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1
##  [889] 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1
##  [926] 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1
##  [963] 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0
## [1000] 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0
## [1037] 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1
## [1074] 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0
## [1111] 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0
## [1148] 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1
## [1185] 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0
## [1222] 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0
## [1259] 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0
## [1296] 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0
## [1333] 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0
## [1370] 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0
## [1407] 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0
## [1444] 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0
## [1481] 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1
## [1518] 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1
## [1555] 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0
## [1592] 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0
## [1629] 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0
## [1666] 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1
## [1703] 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0
## [1740] 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1
## [1777] 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0
## [1814] 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0
## [1851] 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0
## [1888] 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1
## [1925] 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1
## [1962] 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0
## [1999] 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1
## [2036] 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0
## [2073] 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0
## [2110] 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0
## Levels: 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Making the Confusion Matrix}
\NormalTok{cm =}\StringTok{ }\KeywordTok{table}\NormalTok{(test_set[, }\DecValTok{17}\NormalTok{], y_pred)}
\NormalTok{cm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    y_pred
##        0    1
##   0 1107  133
##   1  282  610
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\StringTok{"=====================================KNN====================================="}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "=====================================KNN====================================="
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(lattice)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{confusionMatrix}\NormalTok{(cm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##    y_pred
##        0    1
##   0 1107  133
##   1  282  610
##                                          
##                Accuracy : 0.8053         
##                  95% CI : (0.7879, 0.822)
##     No Information Rate : 0.6515         
##     P-Value [Acc > NIR] : < 2.2e-16      
##                                          
##                   Kappa : 0.5904         
##                                          
##  Mcnemar's Test P-Value : 3.729e-13      
##                                          
##             Sensitivity : 0.7970         
##             Specificity : 0.8210         
##          Pos Pred Value : 0.8927         
##          Neg Pred Value : 0.6839         
##              Prevalence : 0.6515         
##          Detection Rate : 0.5192         
##    Detection Prevalence : 0.5816         
##       Balanced Accuracy : 0.8090         
##                                          
##        'Positive' Class : 0              
## 
\end{verbatim}

\begin{table} 
\begin{center}
\begin{tabular}{|l|c|c|c|} \hline
& Training error & CV error & Public Leaderboard error (if available)\\
  kNN & ... & ... & ...\\
  Ridge & ... & ... & ...\\
  lasso & ... & ... & ...\\
  ElasticNet & ... & ... & ... \\
  random forest & ... & ... \\
  SVM & ... & ... \\
  LDA & ... & ... \\
  QDA & ... & ... \\
  C5.0 & ... & ... \\
  
\hline
\end{tabular}
\end{center}
\caption{Training and CV error of the different models.} \label{tab_res}
\end{table}

\section{Some general comments}

Here are some comments that apply to many of the intermediate reports.

\begin{itemize}
\item
Write your team name on the front page.
\item
Write you own original report, do not follow step by step the exercise but rather think about a useful structure for your own report.
\item 
Use plots and figures, but only those that contain relevant information.
\item
In R plots, make sure that there are labels on each axis, that they are readable, that there is a caption that says what the plot or figure shows, the size of the points/lines is appropriate, the form of the plot is as you want it (e.g., squared), etc.
\item
Use tables, again be careful to describe what the table shows in a caption.
\item 
Avoid lengthy R output of model fits.
\item
Explain what you do: if you use a model, give a brief description in terms of a formula. You don't have to reproduce what we did in lecture, but the 
report should be readable on its own and be consistently written and structured.
\item
In the same vein as the previous comment, if you use notation like AIC, explain what this is and why you use it.
\item
The response is not required to be normal in a linear model (or any other method), it is only assumed to be normal conditional on the predictor values, that is, the residuals should be normal. Slight violation of this does not mean that the model is useless for prediction. Test its performance with CV.
\item
Careful with excluding predictors, even if there is high corrlation, there might be additional information. Test with CV.
\item
It is always good to know the difficulty of the problem. One can assess this by starting with simple benchmark models like the overall mean, kNN, LM, etc. In the report you should try these models and report their errors, this gives you also a feeling about what type of model performs well on this data. You don't have
to report the errors 20 different LMs with interactions, just choose the relevant models (e.g., the best LM with interactions).
\item
If you use CV (and you should), describe what you do exactly. Make sure to use the same CV for all models to make them comparable.
\item
Try to use Latex or RMarkdown, it looks better!
\end{itemize}

\section{Some tests}

Hello test

\end{document}
